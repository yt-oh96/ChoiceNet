{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ChoiceNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPYP8cAcNnFSAp9kZDwr2LL"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-rEvDt64VRWt","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"oMp2H62SpnW7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnsBAqeMPtPg","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import math\n","import torch.nn.functional as F\n","class ChoiceNet(nn.Module):\n","  def __init__(self, feature, y_dim, num_mixture, feature_dim, logSigmaZval, tau_inv, pi1_bias):\n","    self.feature = feature\n","    self.y_dim = y_dim\n","    self.num_mixture = num_mixture\n","    self.feature_dim = feature_dim\n","    self.logSigmaZval = logSigmaZval\n","    self.tau_inv = tau_inv\n","    self.pi1_bias = pi1_bias\n","    #self.USE_GAT = USE_GAT\n","    \n","    \n","    self.fc_feature_dim = nn.Linear(7*7*64, self.feature_dim)\n","    self.fc_num_mixture = nn.Linear(self.feature_dim, num_mixture)\n","\n","    self.fc_var_raw = nn.Linear(self.feature_dim, self.y_dim)\n","\n","    self.fc_pi_logits = nn.Linear(self.feature_dim, self.num_mixture)\n","  def forward(self, x):\n","    x = self.feature(x)\n","    x = x.view(x.size(0),-1) # flatten\n","    feature = self.fc_feature_dim(x) # feature, h\n","    rho_raw = self.fc_num_mixture(feature)\n","    rho_temp = F.sigmoid(rho_raw)\n","    \n","    rho = torch.cat([rho_temp[:, 0:1]*0.0 + rho_ref, rho_temp[:, 1:]], axis=1) # rho(h)=rho1~rhoK, rho_ref=1\n","\n","    Q = self.feature_dim\n","    num_data = x.size()[0]\n","\n","    muW_tile, muZ_tile, sigmaW_tile, sigmaZ_tile = make_sample(Q, num_data) #make_sample\n","\n","    samplerList = cholesky(num_mixture, Q, rho, num_data, muW_tile, sigmaW_tile, muZ_tile, sigmaZ_tile) # cholesky #[K*N*Q*D]\n","\n","    wSample = samplerList.permute(1,3,0,2) #[N*D*K*Q]\n","\n","    #K mean mixtures\n","    wTemp = wSample.view(N, self.num_mixture*self.y_dim, Q) #[N*DK*Q]\n","    featRsh = feature.view(N, Q, 1)\n","     \n","    _mu = torch.matmul(wTemp, reatRsh) #[N*DK*1]\n","    mu = _mu.view(N, self.y_dim, self.num_mixture)\n","\n","    ### Add bias to mu (after)\n","\n","    #K var mixtures\n","    logvar_raw = self.fc_var_raw(feature) #[N*D]\n","    var_raw = torch.exp(logvar_raw)\n","    var_tile = var_raw[:, :, None].repeat(1, 1, self.num_mixture)\n","    rho_tile = rho[:, None, :].repeat(1, self.y_dim, 1)\n","    tau_inv = self.tau_inv\n","    var = (1.0 - torch.pow(rho_tile))*var_tile + tau_inv\n","\n","    # Weight allocation probability pi [N*K]\n","    pi_logits = self.fc_pi_logits(feature) #[N*K]\n","    pi_temp = torch.nn.Softmax(pi_logits, dim=1)\n","\n","    pi_temp = torch.cat(pi_temp[:, 0:1] + self.pi1_bias, pi_temp[:, 1:], axis=1)\n","\n","    pi = torch.nn.softmax(pi_temp, dim=1)\n","\n","    \n","\n","  def cholesky(self, num_mixture, Q, rho, num_data, muW_tile, sigmaW_tile, muZ_tile, sigmaZ_tile):\n","    samplerList = []\n","    for mix_idx in range(self.num_mixture):\n","      rho_j = rho[:, mix_idx : mix_dix+1]\n","      rho_tile = rho_j[:,:,None].repeat(1, Q, self.y_dim)\n","      \n","      epsW = torch.nn.init.normal_(torch.empty(num_data, Q, self.y_dim), mean = 0, std = 1)\n","      W = muW_tile + torch.sqrt(sigmaW_tile)*epsW\n","      \n","      epsZ = torch.nn.init.normal_(torch.empty(num_data, Q, self.y_dim), mean = 0, std = 1)\n","      Z = muZ_tile + torch.sqrt(sigmaZ_tile)*epsZ\n","\n","      #Cholesky\n","      Y = rho_tile*muW_tile + (1.0 - torch.pow(rho_tile,2))\\\n","                               *(rho_tile*torch.sqrt(sigmaZ_tile)/torch.sqrt(sigmaW_tile)\\\n","                                 *(W - muW_tile) + Z*torch.sqrt(1 - torch.pow(rho_tile,2)))\n","      \n","      samplerList.append(Y)\n","    return torch.stack(samplerList)\n","  def make_sample(self, Q, num_data):\n","    N = num_data\n","\n","    muW = torch.nn.init.normal_(torch.empty(Q, self.y_dim), mean = 0.0, std = 0.1)\n","    logSigmaW = torch.nn.init.constant_(torch.empty(Q, self.y_dim), -3.0)\n","    \n","    myZ = torch.zeros(Q, self.y_dim)\n","    logSigmaZ = torch.nn.init.constant_(torch.empty(Q, self.y_dim), self.logSigmaZval)\n","    \n","    muW_tile = muW[None, :, :].repeat(N,1,1)\n","    sigmaW_tile = torch.exp(logSigmaW[None,:,:].reapeat(N,1,1))\n","    \n","    muZ_tile = tile(muZ[None, :, :].repeat(N,1,1))\n","    sigmaZ_tile = torch.exp(logSigmaZ[None,:,:].reapeat(N,1,1))\n","\n","    return muW_tile, muZ_tile, sigmaW_tile, sigmaZ_tile\n","\n","    \n","\n","    #muW_tile = \n","def make_layers(self, in_channels, h_dim, filter_size, max_pools, activation, batch_norm =False):\n","    layers = []\n","\n","    #mnist\n","    for h_idx in range(len(h_dim)):\n","      fs = filter_size[h_idx]\n","      hidden = h_dim[h_idx]\n","      conv2d = nn.Conv2d(in_channels, hidden, kernel_size = fs, padding = 1)\n","      if batch_norm:\n","        layers += [conv2d, nn.BatchNorm2d(hidden), nn.ReLU()]\n","      else:\n","        layers += [conv2d, nn.ReLU()]\n","      in_channels = hidden\n","     \n","      max_pool = max_pools[h_idx]\n","      if max_pool > 1:\n","        layers += [nn.Maxpool2d(max_pool, max_pool)]\n","    return nn.Sequential(*layers)\n","\n","\n","def ChoiceNet_Mnist():\n","  return ChoiceNet(make_layers(in_channels, h_dim, filter_size, max_pools, activation, batch_norm ),\n","                   num_mixture)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0ChmT6jPyXN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}