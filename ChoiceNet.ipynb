{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ChoiceNet_0407.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOwsGKBfqU5xujJTrBcSYj2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b0704a1bdd3146cea0168dfb0e171cc2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_30f8b415dcfb4c47a72bd0bb1054302e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_54dcd598ae0a48fab206bb7ed01c831a","IPY_MODEL_3a4e8768904f4eba9178de3297449d63"]}},"30f8b415dcfb4c47a72bd0bb1054302e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"54dcd598ae0a48fab206bb7ed01c831a":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_301b2c33278a49c39cd028641a13452c","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_efdb7076190249cfa2454bc9472aece2"}},"3a4e8768904f4eba9178de3297449d63":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c78fdaf90c964b9db929f72ac60295da","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9920512/? [00:03&lt;00:00, 3111560.62it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_726fba87050e4eb8bbe4026534466ff2"}},"301b2c33278a49c39cd028641a13452c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"efdb7076190249cfa2454bc9472aece2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c78fdaf90c964b9db929f72ac60295da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"726fba87050e4eb8bbe4026534466ff2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"284425bf6e89498da345720d6073dc44":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e43f49798adf4577b48890193ba5e647","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a9754c4d8aa047f1944bea2341ad9115","IPY_MODEL_4b1b2d1d1a194680a69c9c6bae53b3cb"]}},"e43f49798adf4577b48890193ba5e647":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a9754c4d8aa047f1944bea2341ad9115":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5017d1279b4843cfb620f598129fcd0c","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_49d03dcea96a4d28aafdf9bc855244e1"}},"4b1b2d1d1a194680a69c9c6bae53b3cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_25d7b679822c463a98f3636849d78115","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 32768/? [00:00&lt;00:00, 104202.12it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3e7acdeb8c1f4cc8b2691389abed1382"}},"5017d1279b4843cfb620f598129fcd0c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"49d03dcea96a4d28aafdf9bc855244e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"25d7b679822c463a98f3636849d78115":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3e7acdeb8c1f4cc8b2691389abed1382":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ae4fa6438d064d4295513f174aebea74":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f7d41b79b4ad43b49f4d146ad5fd5a46","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cafbeab2e2cc4a74a84c0f865dc49f72","IPY_MODEL_f82b33e06cab4f93bb59df19a9b024f8"]}},"f7d41b79b4ad43b49f4d146ad5fd5a46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cafbeab2e2cc4a74a84c0f865dc49f72":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e85bad47cfd44158a30c4cb0ebed4039","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3a9734019bbd48adb7acf79c3db62d22"}},"f82b33e06cab4f93bb59df19a9b024f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3ad1af2a878449f89efb8658524e468e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1654784/? [00:01&lt;00:00, 1348573.83it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4ba4cd793aa84513badc21cf628fd964"}},"e85bad47cfd44158a30c4cb0ebed4039":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3a9734019bbd48adb7acf79c3db62d22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ad1af2a878449f89efb8658524e468e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4ba4cd793aa84513badc21cf628fd964":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c34845cc34da46e5b5fd56994ec70748":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_10bea65dc55c4fdca5dd5239ffa65e86","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_577c6ff9350848038208645767cc56c2","IPY_MODEL_bd2de0fc89e148ce88c0969b1e1f7b48"]}},"10bea65dc55c4fdca5dd5239ffa65e86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"577c6ff9350848038208645767cc56c2":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fa4793644de8440281bf9806e38b6071","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_aa6ef96e5d014d3d93f68f89e65084af"}},"bd2de0fc89e148ce88c0969b1e1f7b48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bd6a15b3204f4121bb539164c11b6a9a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 8192/? [00:00&lt;00:00, 20847.14it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ee2929dd9bba47dd91227db5ecd3cc48"}},"fa4793644de8440281bf9806e38b6071":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"aa6ef96e5d014d3d93f68f89e65084af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bd6a15b3204f4121bb539164c11b6a9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ee2929dd9bba47dd91227db5ecd3cc48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"rg2winTipEgC","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import math\n","import torch.nn.functional as F\n","import argparse\n","import torchvision\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import math\n","from torchsummary import summary"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cGsCvtJU-DBo","colab_type":"code","colab":{}},"source":["class ResBlock(nn.Module):\n","  def __init__(self):\n","    super(ResBlock, self).__init__()\n","\n","    self.conv1 = nn.Conv2d(1,64,3,padding=1)\n","    self.bn1 = nn.BatchNorm2d(64)\n","    self.relu1 = nn.ReLU()\n","    self.maxpool1 = nn.MaxPool2d(2)\n","    self.conv2 = nn.Conv2d(64,64,3,padding=1)\n","    self.bn2 = nn.BatchNorm2d(64)\n","    self.relu2 = nn.ReLU()\n","    self.maxpool2 = nn.MaxPool2d(2)\n","  \n","  def forward(self,x):\n","    identity = x\n","\n","    out = self.conv1(x)\n","    out = self.bn1(out)\n","    out += identity\n","    out = self.relu1(out)\n","    out = self.maxpool1(out)\n","\n","    identity = out\n","\n","    out = self.conv2(out)\n","    out = self.bn2(out)\n","    out += identity\n","    out = self.relu2(out)\n","    out = self.maxpool2(out)\n","\n","    return out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LlFr1_RbkZ_u","colab_type":"code","colab":{}},"source":["class ChoiceNet(nn.Module):\n","  def __init__(self, y_dim, num_mixture, feature_dim, logSigmaZval, tau_inv, pi1_bias):\n","    super(ChoiceNet,self).__init__()\n","    #self.backbone = backbone\n","    self.y_dim = y_dim\n","    self.num_mixture = num_mixture\n","    self.feature_dim = feature_dim\n","    self.logSigmaZval = logSigmaZval\n","    self.tau_inv = tau_inv\n","    self.pi1_bias = pi1_bias\n","    self.rho_ref = 0.95\n","    #self.USE_GAT = USE_GAT\n","    \n","    self.ResBlock = ResBlock()\n","\n","    # self.back_bone = nn.Sequential(\n","    #     nn.Conv2d(1,64,3,padding=1),\n","    #     nn.ReLU(),\n","    #     nn.MaxPool2d(2),\n","    #     nn.Conv2d(64,64,3,padding=1),\n","    #     nn.ReLU(),\n","    #     nn.MaxPool2d(2)   \n","    # )\n","    \n","    self.fc_feature_dim = nn.Linear(7*7*64, self.feature_dim)\n","    self.fc_num_mixture = nn.Linear(self.feature_dim, num_mixture)\n","\n","    self.fc_var_raw = nn.Linear(self.feature_dim, self.y_dim)\n","\n","    self.fc_pi_logits = nn.Linear(self.feature_dim, self.num_mixture)\n","\n","  def forward(self, x):\n","    #x = self.back_bone(x)\n","    x = self.ResBlock(x)\n","    x = x.view(x.size(0),-1) # flatten\n","    #print('x : ',x)\n","    self.feature = self.fc_feature_dim(x) # feature, h\n","    #print(self.feature.size())\n","    \n","    with torch.enable_grad():\n","      rho_raw = self.fc_num_mixture(self.feature)\n","      rho_temp = F.sigmoid(rho_raw)\n","      rho = torch.cat([rho_temp[:, 0:1]*0.0 + self.rho_ref, rho_temp[:, 1:]], axis=1) # rho(h)=rho1~rhoK, rho_ref=1\n","\n","      #print('rho_raw : ',rho_raw)\n","      #print('rho_temp : ',rho_temp)\n","      Q = self.feature_dim\n","      num_data = x.size()[0]\n","\n","    #make_sample\n","      muW_tile, muZ_tile, sigmaW_tile, sigmaZ_tile = self.make_sample(Q, num_data) \n","      # print('muW_tile : ',muW_tile, \n","      #       '\\nmuZ_tile : ',muZ_tile, \n","      #       '\\nsigmaW_tile : ',sigmaW_tile, \n","      #       '\\nsigmaZ_tile : ',sigmaZ_tile)\n","    \n","    # cholesky #[K*N*Q*D] #W_bar\n","    #branch_2\n","      samplerList = self.cholesky(self.num_mixture, Q, rho, num_data, muW_tile, sigmaW_tile, muZ_tile, sigmaZ_tile)\n","      wSample = samplerList.permute(1,3,0,2) #[N*D*K*Q]\n","\n","    #K mean mixtures\n","      wTemp = wSample.contiguous().view(num_data, self.num_mixture*self.y_dim, Q)\n","      featRsh = self.feature.view(num_data, Q, 1)\n","      _mu = torch.matmul(wTemp, featRsh) #[N*DK*1]\n","      mu = _mu.view(num_data, self.y_dim, self.num_mixture)\n","\n","    ### Add bias to mu (after)\n","\n","    #K var mixtures #(6) \n","    #branch_3\n","      logvar_raw = self.fc_var_raw(self.feature) #[N*D]\n","      var_raw = torch.exp(logvar_raw)\n","      #print('logvar_raw:',logvar_raw)\n","      var_tile = var_raw.unsqueeze(-1).repeat(1, 1, self.num_mixture) #N*D*K\n","      rho_tile = rho.unsqueeze(1).repeat(1, self.y_dim, 1)#N,D,K\n","      tau_inv = self.tau_inv\n","      var = (1.0 - torch.pow(rho_tile,2))*var_tile + tau_inv\n","\n","    # Weight allocation probability pi [N*K] # pi_k = softmax()_k\n","    #branch_1\n","      pi_logits = self.fc_pi_logits(self.feature) #[N*K]\n","      pi_temp = F.softmax(pi_logits, dim=1)\n","      pi_temp = torch.cat((pi_temp[:, 0:1] + self.pi1_bias, pi_temp[:, 1:]), axis=1)\n","      pi = F.softmax(pi_temp, dim=1)\n","\n","    return rho, mu, var, pi\n","    \n","\n","  def make_sample(self, Q, num_data):\n","    N = num_data\n","\n","    muW = torch.nn.init.normal_(torch.empty(Q, self.y_dim), mean = 0.0, std = 0.1)\n","    #muW = torch.normal(std=0.1,size=(Q, self.y_dim)) # Q*D\n","    logSigmaW = torch.nn.init.constant_(torch.empty(Q, self.y_dim), -3.0)\n","    #print('logSigmaW : ',logSigmaW)\n","    muZ = torch.zeros(Q, self.y_dim) # Q*D\n","    logSigmaZ = torch.nn.init.constant_(torch.empty(Q, self.y_dim), self.logSigmaZval)\n","    #print('logSigmaZ : ',logSigmaZ)\n","    muW_tile = muW.unsqueeze(0).repeat(N,1,1) # N*Q*D\n","    sigmaW_tile = torch.exp(logSigmaW.unsqueeze(0).repeat(N,1,1)) #N*Q*D\n","    \n","    muZ_tile = muZ.unsqueeze(0).repeat(N,1,1)\n","    sigmaZ_tile = torch.exp(logSigmaZ.unsqueeze(0).repeat(N,1,1))\n","\n","    return muW_tile, muZ_tile, sigmaW_tile, sigmaZ_tile\n","\n","  def cholesky(self, num_mixture, Q, rho, num_data, muW_tile, sigmaW_tile, muZ_tile, sigmaZ_tile):\n","    samplerList = []\n","    for mix_idx in range(self.num_mixture):\n","      rho_j = rho[:, mix_idx : mix_idx+1] # N*1\n","      rho_tile = rho_j.unsqueeze(-1).repeat(1, Q, self.y_dim) # N*Q*D\n","      \n","      epsW = torch.randn(num_data, Q, self.y_dim, dtype=torch.float) #mean=0, std=1\n","      W = muW_tile + torch.sqrt(sigmaW_tile)*epsW\n","      \n","      epsZ = torch.randn(num_data, Q, self.y_dim, dtype=torch.float)\n","      Z = muZ_tile + torch.sqrt(sigmaZ_tile)*epsZ\n","\n","      #Cholesky\n","      Y = rho_tile*muW_tile + (1.0 - torch.pow(rho_tile,2))\\\n","                               *(rho_tile*torch.sqrt(sigmaZ_tile)/torch.sqrt(sigmaW_tile)\\\n","                                 *(W - muW_tile) + Z*torch.sqrt(1 - torch.pow(rho_tile,2)))\n","                               \n","      #print('Y: ' ,Y)\n","      samplerList.append(Y)\n","    return torch.stack(samplerList)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DMyL8HLSpKMv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wo9tzqTC_Ql4","colab_type":"code","colab":{}},"source":["def ChoiceNet_Mnist():\n","  return ChoiceNet(y_dim, num_mixture, feature_dim, logSigmaZval, tau_inv, pi1_bias)\n","  # return ChoiceNet(make_layers(in_channels, h_dim, filter_size, max_pools, activation, batch_norm ),\n","  #                  y_dim, num_mixture, feature_dim, logSigmaZval, tau_inv, pi1_bias)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"29xJVSP5fhjB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MR6pd3-0pNoR","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQJTMfZ3cBiC","colab_type":"code","colab":{}},"source":["def train(model,epoch):\n","  model.train()\n","  for batch_idx, (data,target) in enumerate(train_loader):\n","    if args['cuda']:\n","      data, target = data.cuda(), target.cuda()\n","    \n","    temp = torch.LongTensor(1000,1).random_() % 10\n","    y_onehot = torch.FloatTensor(1000,10)\n","\n","    y_onehot.zero_()\n","    y_onehot.scatter_(1, temp, 1)\n","    \n","    #data, target = Variable(data), Variable(y_onehot)\n","    data, target = Variable(data), Variable(y_onehot)\n","    #GRAD_CLIP = True, USE_SGD = False\n","\n","    optimizer.zero_grad()\n","\n","    rho, mu, var, pi = model(data)\n","    #print('rho : ',rho,'\\nmu : ', mu,'\\nvar :' ,var,'\\npi : ', pi)\n","    loss, acc = MDNloss(len(data), rho, mu, var, pi, target, y_dim=10, num_mixture=10, logsumexp_coef= 1e-2, kl_reg_coef= 1e-4).forward()\n","    \n","    loss.backward()\n","\n","    print(acc)\n","\n","    optimizer.step()\n","\n","    if batch_idx%args['log_interval'] == 0:\n","      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","          epoch, batch_idx * len(data), len(train_loader.dataset),\n","          100. *batch_idx/len(train_loader), loss.data\n","      ))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zS3dvJTbGQ_G","colab_type":"code","colab":{}},"source":["def test():\n","  model.eval()\n","  test_loss = 0\n","  correct = 0\n","  for data,target in test_loader:\n","    if args['cuda']:\n","      data, target = data.cuda(), target.cuda()\n","    \n","    temp = torch.LongTensor(1000,1).random_() % 10\n","    y_onehot = torch.FloatTensor(1000,10)\n","\n","    y_onehot.zero_()\n","    y_onehot.scatter_(1, temp, 1)\n","    \n","    data, target = Variable(data), Variable(y_onehot)\n","\n","    rho, mu, var, pi = model(data)\n","    \n","\n","    loss , acc = MDNloss(len(data), rho, mu, var, pi, target, y_dim=10, num_mixture=10, logsumexp_coef= 1e-2, kl_reg_coef= 1e-4).forward()\n","    test_loss += loss\n","    correct += acc#accuracy(pi,mu, len(data) ,target, y_dim=10)\n","  \n","  loss /= len(test_loader.dataset)\n","  print('\\nTest set : Average loss : {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","      loss, correct, len(test_loader.dataset),\n","      100.*correct/len(test_loader.dataset)))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLZbj1dTpUSh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xqZgxNsYkx53","colab":{}},"source":["def train_overfit(epoch):\n","  model.train()\n","  for batch_idx, (data,target) in enumerate(train_loader):\n","    if args['cuda']:\n","        data, target = data.cuda(), target.cuda()\n","      \n","    temp = torch.LongTensor(1000,1).random_() % 10\n","    y_onehot = torch.FloatTensor(1000,10)\n","\n","    y_onehot.zero_()\n","    y_onehot.scatter_(1, temp, 1)\n","      \n","      #data, target = Variable(data), Variable(y_onehot)\n","    data, target = Variable(data[0:2]), Variable(y_onehot[0:2])\n","    #print(data, target)\n","    for i in range(100):\n","      \n","      #GRAD_CLIP = True, USE_SGD = False\n","\n","      optimizer.zero_grad()\n","\n","      rho, mu, var, pi = model(data)\n","      #print('rho : ',rho,'\\nmu : ', mu,'\\nvar :' ,var,'\\npi : ', pi)\n","      loss, acc = MDNloss(len(data), rho, mu, var, pi, target, y_dim=10, num_mixture=5, logsumexp_coef= 1e-2, kl_reg_coef= 1e-4).forward()\n","      \n","      loss.backward()\n","\n","      print(acc)\n","\n","      optimizer.step()\n","\n","      if batch_idx%args['log_interval'] == 0:\n","        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","            epoch, batch_idx * len(data), len(train_loader.dataset),\n","            100. *batch_idx/len(train_loader), loss.data\n","        ))\n","    break\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"deWAxCX_nHvk","colab_type":"code","colab":{}},"source":["def test(model):\n","  model.eval()\n","  test_loss = 0\n","  correct = 0\n","  for data,target in test_loader:\n","    if args['cuda']:\n","      data, target = data.cuda(), target.cuda()\n","    \n","    temp = torch.LongTensor(1000,1).random_() % 10\n","    y_onehot = torch.FloatTensor(1000,10)\n","\n","    y_onehot.zero_()\n","    y_onehot.scatter_(1, temp, 1)\n","    \n","    data, target = Variable(data), Variable(y_onehot)\n","\n","    rho, mu, var, pi = model(data)\n","    \n","\n","    loss , acc = MDNloss(len(data), rho, mu, var, pi, target, y_dim=10, num_mixture=10, logsumexp_coef= 1e-2, kl_reg_coef= 1e-4).forward()\n","    test_loss += loss\n","    correct += acc#accuracy(pi,mu, len(data) ,target, y_dim=10)\n","  \n","  loss /= len(test_loader.dataset)\n","  print('\\nTest set : Average loss : {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","      loss, correct, len(test_loader.dataset),\n","      100.*correct/len(test_loader.dataset)))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"smXV2S0Yorbb","colab_type":"code","colab":{}},"source":["class MDNloss(nn.Module):\n","  def __init__(self, num_data, rho, mu, var, pi, target, y_dim, num_mixture, logsumexp_coef, kl_reg_coef):\n","    super(MDNloss, self).__init__()\n","    self.num_data = num_data\n","    self.rho = rho #N*K\n","    self.mu = mu #N*D*K\n","    self.var = var #N*D*K\n","    self.pi = pi #N*K\n","    self.target = target\n","    self.y_dim = y_dim\n","    self.num_mixture = num_mixture\n","    self.logsumexp_coef = logsumexp_coef\n","    self.kl_reg_coef = kl_reg_coef\n","\n","    #print(self.var)\n","\n","    self.yhat = self.mu + torch.sqrt(self.var)*torch.randn(self.num_data, self.y_dim, self.num_mixture)\n","\n","  def forward(self):\n","    #with torch.no_grad():\n","    target_tile = self.target.unsqueeze(-1).repeat(1, 1, self.num_mixture)# N*D*K\n","    pi_tile = self.pi.unsqueeze(1).repeat(1, self.y_dim, 1) # N*D*K\n","\n","    yhat_normalized = F.softmax(self.yhat, dim=1)\n","    _loss_fit = torch.sum(-pi_tile*yhat_normalized*target_tile, axis=[1,2])\n","    loss_fit = torch.mean(_loss_fit)\n","\n","    _loss_reg = self.pi*torch.logsumexp(self.yhat,axis=[1])\n","    __loss_reg = torch.sum(_loss_reg,axis=[1])\n","    loss_reg = self.logsumexp_coef*torch.mean(__loss_reg)\n","\n","    _eps = 1e-8\n","    _kl_reg = self.kl_reg_coef*torch.sum(-self.rho*(torch.log(self.pi+_eps) - torch.log(self.rho+_eps)), axis=1)\n","    kl_reg = torch.mean(_kl_reg)\n","    # prob = self.pi*self.g_p(self.var,self.mu,self.target)\n","    # nll = -torch.log(torch.sum(prob,dim=1))\n","\n","    acc = self.acc()\n","\n","    print(loss_fit.item() ,'+', loss_reg.item() ,'+', kl_reg.item(),'=',torch.mean(loss_fit + loss_reg + kl_reg))\n"," \n","    #print('yhat:',torch.argmax(yhat_normalized,dim=1))\n","    #print('pi : ', self.pi)\n","    #print('target:',torch.argmax(self.target,dim=1))\n","    return torch.mean(loss_fit + loss_reg + kl_reg), acc\n","\n","  def acc(self):\n","    #print(self.pi)\n","    y = self.yhat[:,:,0] #N*D\n","    #print('y',torch.argmax(y,dim=1))\n","    #print('target',torch.argmax(self.target,dim=1))\n","    acc = (torch.argmax(y,dim=1) == torch.argmax(self.target,dim=1)).sum().item()/y.size()[0]\n","\n","    return acc\n","    #self.target #N*D\n","\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eiliGvi5pXlr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WgsBQTzolR0y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":332,"referenced_widgets":["b0704a1bdd3146cea0168dfb0e171cc2","30f8b415dcfb4c47a72bd0bb1054302e","54dcd598ae0a48fab206bb7ed01c831a","3a4e8768904f4eba9178de3297449d63","301b2c33278a49c39cd028641a13452c","efdb7076190249cfa2454bc9472aece2","c78fdaf90c964b9db929f72ac60295da","726fba87050e4eb8bbe4026534466ff2","284425bf6e89498da345720d6073dc44","e43f49798adf4577b48890193ba5e647","a9754c4d8aa047f1944bea2341ad9115","4b1b2d1d1a194680a69c9c6bae53b3cb","5017d1279b4843cfb620f598129fcd0c","49d03dcea96a4d28aafdf9bc855244e1","25d7b679822c463a98f3636849d78115","3e7acdeb8c1f4cc8b2691389abed1382","ae4fa6438d064d4295513f174aebea74","f7d41b79b4ad43b49f4d146ad5fd5a46","cafbeab2e2cc4a74a84c0f865dc49f72","f82b33e06cab4f93bb59df19a9b024f8","e85bad47cfd44158a30c4cb0ebed4039","3a9734019bbd48adb7acf79c3db62d22","3ad1af2a878449f89efb8658524e468e","4ba4cd793aa84513badc21cf628fd964","c34845cc34da46e5b5fd56994ec70748","10bea65dc55c4fdca5dd5239ffa65e86","577c6ff9350848038208645767cc56c2","bd2de0fc89e148ce88c0969b1e1f7b48","fa4793644de8440281bf9806e38b6071","aa6ef96e5d014d3d93f68f89e65084af","bd6a15b3204f4121bb539164c11b6a9a","ee2929dd9bba47dd91227db5ecd3cc48"]},"outputId":"102d78c5-639d-4ec1-f997-8f1402999f88","executionInfo":{"status":"ok","timestamp":1586331989750,"user_tz":-540,"elapsed":6956,"user":{"displayName":"‍오영택[ 대학원석·박사통합과정휴학 / 인공지능학과 ]","photoUrl":"","userId":"14804494411714270071"}}},"source":["args = {}\n","kwargs = {}\n","args['batch_size'] = 1000\n","args['test_batch_size'] = 1000\n","args['epochs'] = 50\n","args['lr'] = 1e-5\n","args['momentum'] = 0.5\n","\n","args['seed'] = 1\n","args['log_interval'] = 10\n","args['cuda'] = False\n","\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=True, download=True,\n","                   transform=transforms.Compose([\n","                                                 transforms.ToTensor(),\n","                                                 transforms.Normalize((0.1307,), (0.3081,))\n","                   ])),\n","                   batch_size = args['batch_size'], shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=False, download=True,\n","                   transform=transforms.Compose([\n","                                                 transforms.ToTensor(),\n","                                                 transforms.Normalize((0.1307,), (0.3081,))\n","                   ])),\n","                   batch_size = args['test_batch_size'], shuffle=True, **kwargs)\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0704a1bdd3146cea0168dfb0e171cc2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"284425bf6e89498da345720d6073dc44","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae4fa6438d064d4295513f174aebea74","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c34845cc34da46e5b5fd56994ec70748","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n","Processing...\n","Done!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QfmYs5WEpZmW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2O2Ls3WSpXCN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"1976f4f0-c3c0-428a-b7bd-b494ebd9af1f","executionInfo":{"status":"ok","timestamp":1586331989751,"user_tz":-540,"elapsed":6948,"user":{"displayName":"‍오영택[ 대학원석·박사통합과정휴학 / 인공지능학과 ]","photoUrl":"","userId":"14804494411714270071"}}},"source":["from torch.utils.tensorboard import SummaryWriter\n","import matplotlib.pyplot as plt\n","\n","def matplotlib_imshow(img, one_channel=False):\n","    if one_channel:\n","        img = img.mean(dim=0)\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    if one_channel:\n","        plt.imshow(npimg, cmap=\"Greys\")\n","    else:\n","        plt.imshow(np.transpose(npimg, (1, 2, 0)))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["\n","\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zjm-mleWrcUN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":269},"outputId":"c1dde82f-c030-4f22-b0f0-5e9585269fda","executionInfo":{"status":"ok","timestamp":1586331992312,"user_tz":-540,"elapsed":9502,"user":{"displayName":"‍오영택[ 대학원석·박사통합과정휴학 / 인공지능학과 ]","photoUrl":"","userId":"14804494411714270071"}}},"source":["writer = SummaryWriter('runs/ChoiceNet_mnist_experiment_1')\n","\n","dataiter = iter(train_loader)\n","images, labels = dataiter.next()\n","\n","# 이미지 그리드를 만듭니다.\n","img_grid = torchvision.utils.make_grid(images)\n","\n","# 이미지를 보여줍니다.\n","matplotlib_imshow(img_grid, one_channel=True)\n","#matplotlib_imshow(img_grid, one_channel=True)\n","\n","# tensorboard에 기록합니다.\n","writer.add_image('ChoiceNet_mnist_images', img_grid)"],"execution_count":12,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAEMAAAD8CAYAAAAsVhnTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"loozs6I6tTYr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"knTcmtjD_uGe","colab_type":"code","outputId":"2fce5add-7666-4be6-ca81-8d27b26e7d33","executionInfo":{"status":"error","timestamp":1586333958320,"user_tz":-540,"elapsed":1815017,"user":{"displayName":"‍오영택[ 대학원석·박사통합과정휴학 / 인공지능학과 ]","photoUrl":"","userId":"14804494411714270071"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["def init_weights(m):\n","  if type(m) == nn.Linear:\n","    torch.nn.init.normal(m.weight)\n","    #torch.nn.init.xavier_normal(m.weight)\n","  if type(m) == nn.Conv2d:\n","    torch.nn.init.normal(m.weight)\n","    #torch.nn.init.xavier_normal(m.weight)\n","\n","model = ChoiceNet(y_dim=10, num_mixture=10, feature_dim=256, logSigmaZval=-2, tau_inv=1e-4, pi1_bias=0.0)\n","#model.apply(init_weights)\n","optimizer = optim.Adam(model.parameters(), lr=args['lr'], weight_decay=1e-5)\n","\n","summary(model,(1,28,28))\n","\n","for name, param in model.named_parameters():\n","  if param.requires_grad:\n","    print (name, param.data.size())\n","for epoch in range(1, args['epochs']+1):\n","  train(model,epoch)\n","  test(model)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 28, 28]             640\n","       BatchNorm2d-2           [-1, 64, 28, 28]             128\n","              ReLU-3           [-1, 64, 28, 28]               0\n","         MaxPool2d-4           [-1, 64, 14, 14]               0\n","            Conv2d-5           [-1, 64, 14, 14]          36,928\n","       BatchNorm2d-6           [-1, 64, 14, 14]             128\n","              ReLU-7           [-1, 64, 14, 14]               0\n","         MaxPool2d-8             [-1, 64, 7, 7]               0\n","          ResBlock-9             [-1, 64, 7, 7]               0\n","           Linear-10                  [-1, 256]         803,072\n","           Linear-11                   [-1, 10]           2,570\n","           Linear-12                   [-1, 10]           2,570\n","           Linear-13                   [-1, 10]           2,570\n","================================================================\n","Total params: 848,606\n","Trainable params: 848,606\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 1.58\n","Params size (MB): 3.24\n","Estimated Total Size (MB): 4.82\n","----------------------------------------------------------------\n","ResBlock.conv1.weight torch.Size([64, 1, 3, 3])\n","ResBlock.conv1.bias torch.Size([64])\n","ResBlock.bn1.weight torch.Size([64])\n","ResBlock.bn1.bias torch.Size([64])\n","ResBlock.conv2.weight torch.Size([64, 64, 3, 3])\n","ResBlock.conv2.bias torch.Size([64])\n","ResBlock.bn2.weight torch.Size([64])\n","ResBlock.bn2.bias torch.Size([64])\n","fc_feature_dim.weight torch.Size([256, 3136])\n","fc_feature_dim.bias torch.Size([256])\n","fc_num_mixture.weight torch.Size([10, 256])\n","fc_num_mixture.bias torch.Size([10])\n","fc_var_raw.weight torch.Size([10, 256])\n","fc_var_raw.bias torch.Size([10])\n","fc_pi_logits.weight torch.Size([10, 256])\n","fc_pi_logits.bias torch.Size([10])\n","-0.10129821300506592 + 0.1202610582113266 + 0.0007869338733144104 = tensor(0.0197, grad_fn=<MeanBackward0>)\n","0.104\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.019750\n","-0.09561517089605331 + 0.11901654303073883 + 0.0008308170363306999 = tensor(0.0242, grad_fn=<MeanBackward0>)\n","0.11\n","-0.0964086726307869 + 0.11786570399999619 + 0.0008830379811115563 = tensor(0.0223, grad_fn=<MeanBackward0>)\n","0.102\n","-0.10133551061153412 + 0.10778830945491791 + 0.0009140598704107106 = tensor(0.0074, grad_fn=<MeanBackward0>)\n","0.117\n","-0.10202625393867493 + 0.11104775965213776 + 0.0009559163590893149 = tensor(0.0100, grad_fn=<MeanBackward0>)\n","0.101\n","-0.10512038320302963 + 0.10703717917203903 + 0.0010112591553479433 = tensor(0.0029, grad_fn=<MeanBackward0>)\n","0.104\n","-0.09817428141832352 + 0.10535455495119095 + 0.001066415454261005 = tensor(0.0082, grad_fn=<MeanBackward0>)\n","0.089\n","-0.1033681333065033 + 0.09872327744960785 + 0.0011180250439792871 = tensor(-0.0035, grad_fn=<MeanBackward0>)\n","0.115\n","-0.09871095418930054 + 0.10118181258440018 + 0.001154131256043911 = tensor(0.0036, grad_fn=<MeanBackward0>)\n","0.124\n","-0.09318950772285461 + 0.0831090435385704 + 0.0012009469792246819 = tensor(-0.0089, grad_fn=<MeanBackward0>)\n","0.076\n","-0.10407406091690063 + 0.08997249603271484 + 0.0012511262902989984 = tensor(-0.0129, grad_fn=<MeanBackward0>)\n","0.106\n","Train Epoch: 1 [10000/60000 (17%)]\tLoss: -0.012850\n","-0.10305698961019516 + 0.08247260749340057 + 0.001296713249757886 = tensor(-0.0193, grad_fn=<MeanBackward0>)\n","0.114\n","-0.09367627650499344 + 0.07799229770898819 + 0.00134201068431139 = tensor(-0.0143, grad_fn=<MeanBackward0>)\n","0.096\n","-0.10366559028625488 + 0.08447986096143723 + 0.0013911465648561716 = tensor(-0.0178, grad_fn=<MeanBackward0>)\n","0.107\n","-0.09567652642726898 + 0.07823950797319412 + 0.0014302211347967386 = tensor(-0.0160, grad_fn=<MeanBackward0>)\n","0.101\n","-0.09646391123533249 + 0.07478217035531998 + 0.0014699609018862247 = tensor(-0.0202, grad_fn=<MeanBackward0>)\n","0.092\n","-0.09660374373197556 + 0.07934483140707016 + 0.001510842703282833 = tensor(-0.0157, grad_fn=<MeanBackward0>)\n","0.085\n","-0.09892058372497559 + 0.06748442351818085 + 0.001549284439533949 = tensor(-0.0299, grad_fn=<MeanBackward0>)\n","0.103\n","-0.09934195131063461 + 0.07200131565332413 + 0.0015829589683562517 = tensor(-0.0258, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10198080539703369 + 0.06615877151489258 + 0.001618090784177184 = tensor(-0.0342, grad_fn=<MeanBackward0>)\n","0.101\n","-0.10326240211725235 + 0.06268339604139328 + 0.0016498748445883393 = tensor(-0.0389, grad_fn=<MeanBackward0>)\n","0.091\n","Train Epoch: 1 [20000/60000 (33%)]\tLoss: -0.038929\n","-0.10458917170763016 + 0.06670474261045456 + 0.0016712447395548224 = tensor(-0.0362, grad_fn=<MeanBackward0>)\n","0.099\n","-0.10121381282806396 + 0.06360844522714615 + 0.0017055930802598596 = tensor(-0.0359, grad_fn=<MeanBackward0>)\n","0.093\n","-0.09758777916431427 + 0.06325192749500275 + 0.0017337336903437972 = tensor(-0.0326, grad_fn=<MeanBackward0>)\n","0.092\n","-0.09933633357286453 + 0.0684848204255104 + 0.0017644771141931415 = tensor(-0.0291, grad_fn=<MeanBackward0>)\n","0.09\n","-0.10111964493989944 + 0.0649016723036766 + 0.0017917382065206766 = tensor(-0.0344, grad_fn=<MeanBackward0>)\n","0.112\n","-0.09486323595046997 + 0.054856691509485245 + 0.0018034930108115077 = tensor(-0.0382, grad_fn=<MeanBackward0>)\n","0.096\n","-0.0974864810705185 + 0.05588538944721222 + 0.0018262562807649374 = tensor(-0.0398, grad_fn=<MeanBackward0>)\n","0.104\n","-0.10386017709970474 + 0.048270490020513535 + 0.0018406608141958714 = tensor(-0.0537, grad_fn=<MeanBackward0>)\n","0.097\n","-0.10335242003202438 + 0.05427481606602669 + 0.0018590650288388133 = tensor(-0.0472, grad_fn=<MeanBackward0>)\n","0.101\n","-0.09633170068264008 + 0.047279294580221176 + 0.001878647250123322 = tensor(-0.0472, grad_fn=<MeanBackward0>)\n","0.082\n","Train Epoch: 1 [30000/60000 (50%)]\tLoss: -0.047174\n","-0.1031765341758728 + 0.06017111986875534 + 0.001891513355076313 = tensor(-0.0411, grad_fn=<MeanBackward0>)\n","0.107\n","-0.09449617564678192 + 0.06421294808387756 + 0.0019086033571511507 = tensor(-0.0284, grad_fn=<MeanBackward0>)\n","0.104\n","-0.09992293268442154 + 0.049242790788412094 + 0.0019273693906143308 = tensor(-0.0488, grad_fn=<MeanBackward0>)\n","0.114\n","-0.10685839504003525 + 0.047120846807956696 + 0.0019457010785117745 = tensor(-0.0578, grad_fn=<MeanBackward0>)\n","0.117\n","-0.09496823698282242 + 0.04463397338986397 + 0.0019482909701764584 = tensor(-0.0484, grad_fn=<MeanBackward0>)\n","0.074\n","-0.09719037264585495 + 0.03905711695551872 + 0.00195059715770185 = tensor(-0.0562, grad_fn=<MeanBackward0>)\n","0.086\n","-0.10118791460990906 + 0.05750592052936554 + 0.001956271706148982 = tensor(-0.0417, grad_fn=<MeanBackward0>)\n","0.093\n","-0.10102928429841995 + 0.06140819564461708 + 0.001974408747628331 = tensor(-0.0376, grad_fn=<MeanBackward0>)\n","0.116\n","-0.09808854758739471 + 0.04973575845360756 + 0.001982608810067177 = tensor(-0.0464, grad_fn=<MeanBackward0>)\n","0.105\n","-0.09558440744876862 + 0.04112263023853302 + 0.0019944943487644196 = tensor(-0.0525, grad_fn=<MeanBackward0>)\n","0.091\n","Train Epoch: 1 [40000/60000 (67%)]\tLoss: -0.052467\n","-0.09764017164707184 + 0.039731066673994064 + 0.0019987665582448244 = tensor(-0.0559, grad_fn=<MeanBackward0>)\n","0.096\n","-0.09162572026252747 + 0.05929373949766159 + 0.00201041460968554 = tensor(-0.0303, grad_fn=<MeanBackward0>)\n","0.093\n","-0.1059584766626358 + 0.04597359523177147 + 0.0020106835290789604 = tensor(-0.0580, grad_fn=<MeanBackward0>)\n","0.1\n","-0.09440308064222336 + 0.0431625135242939 + 0.002015112666413188 = tensor(-0.0492, grad_fn=<MeanBackward0>)\n","0.084\n","-0.10212134569883347 + 0.04719429835677147 + 0.002029093913733959 = tensor(-0.0529, grad_fn=<MeanBackward0>)\n","0.096\n","-0.10714477300643921 + 0.041000980883836746 + 0.002030301606282592 = tensor(-0.0641, grad_fn=<MeanBackward0>)\n","0.106\n","-0.0988101065158844 + 0.061077650636434555 + 0.0020367580000311136 = tensor(-0.0357, grad_fn=<MeanBackward0>)\n","0.089\n","-0.09924234449863434 + 0.041605815291404724 + 0.0020355205051600933 = tensor(-0.0556, grad_fn=<MeanBackward0>)\n","0.097\n","-0.09647670388221741 + 0.04844391345977783 + 0.0020377798937261105 = tensor(-0.0460, grad_fn=<MeanBackward0>)\n","0.097\n","-0.09553495049476624 + 0.038407985121011734 + 0.002037752652540803 = tensor(-0.0551, grad_fn=<MeanBackward0>)\n","0.093\n","Train Epoch: 1 [50000/60000 (83%)]\tLoss: -0.055089\n","-0.10623174905776978 + 0.043336935341358185 + 0.002042093314230442 = tensor(-0.0609, grad_fn=<MeanBackward0>)\n","0.104\n","-0.09870124608278275 + 0.05741000175476074 + 0.0020410320721566677 = tensor(-0.0393, grad_fn=<MeanBackward0>)\n","0.098\n","-0.10111819207668304 + 0.05669153109192848 + 0.002049677539616823 = tensor(-0.0424, grad_fn=<MeanBackward0>)\n","0.107\n","-0.10516542196273804 + 0.04776337370276451 + 0.0020465843845158815 = tensor(-0.0554, grad_fn=<MeanBackward0>)\n","0.106\n","-0.11135273426771164 + 0.06365346908569336 + 0.002050970681011677 = tensor(-0.0456, grad_fn=<MeanBackward0>)\n","0.106\n","-0.09794090688228607 + 0.04819827526807785 + 0.0020573444198817015 = tensor(-0.0477, grad_fn=<MeanBackward0>)\n","0.101\n","-0.10430862754583359 + 0.048950355499982834 + 0.0020516186486929655 = tensor(-0.0533, grad_fn=<MeanBackward0>)\n","0.096\n","-0.09702714532613754 + 0.04879403114318848 + 0.0020549411419779062 = tensor(-0.0462, grad_fn=<MeanBackward0>)\n","0.103\n","-0.09209996461868286 + 0.04746745899319649 + 0.002053615404292941 = tensor(-0.0426, grad_fn=<MeanBackward0>)\n","0.09\n","-0.09954135864973068 + 0.05054669827222824 + 0.002050162525847554 = tensor(-0.0469, grad_fn=<MeanBackward0>)\n","-0.10439885407686234 + 0.04646936431527138 + 0.002053128322586417 = tensor(-0.0559, grad_fn=<MeanBackward0>)\n","-0.09995537251234055 + 0.04804633930325508 + 0.00205135066062212 = tensor(-0.0499, grad_fn=<MeanBackward0>)\n","-0.10195790231227875 + 0.045933254063129425 + 0.0020549676846712828 = tensor(-0.0540, grad_fn=<MeanBackward0>)\n","-0.09475234895944595 + 0.04425371438264847 + 0.002050550188869238 = tensor(-0.0484, grad_fn=<MeanBackward0>)\n","-0.09026047587394714 + 0.04749329760670662 + 0.0020515662617981434 = tensor(-0.0407, grad_fn=<MeanBackward0>)\n","-0.10090994834899902 + 0.03535780310630798 + 0.00205190759152174 = tensor(-0.0635, grad_fn=<MeanBackward0>)\n","-0.10793502628803253 + 0.0400722362101078 + 0.0020549986511468887 = tensor(-0.0658, grad_fn=<MeanBackward0>)\n","-0.09499144554138184 + 0.0544511079788208 + 0.0020558517426252365 = tensor(-0.0385, grad_fn=<MeanBackward0>)\n","-0.10337197780609131 + 0.043717604130506516 + 0.002054167678579688 = tensor(-0.0576, grad_fn=<MeanBackward0>)\n","\n","Test set : Average loss : -0.0000, Accuracy: 0.9949999999999999/10000 (0%)\n","\n","-0.09666311740875244 + 0.044206004589796066 + 0.002047579037025571 = tensor(-0.0504, grad_fn=<MeanBackward0>)\n","0.102\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: -0.050410\n","-0.10869371145963669 + 0.05167244002223015 + 0.0020554775837808847 = tensor(-0.0550, grad_fn=<MeanBackward0>)\n","0.107\n","-0.09716763347387314 + 0.04922500625252724 + 0.002055322751402855 = tensor(-0.0459, grad_fn=<MeanBackward0>)\n","0.093\n","-0.09734343737363815 + 0.051317472010850906 + 0.002050243318080902 = tensor(-0.0440, grad_fn=<MeanBackward0>)\n","0.089\n","-0.0972335934638977 + 0.032370638102293015 + 0.002052613999694586 = tensor(-0.0628, grad_fn=<MeanBackward0>)\n","0.091\n","-0.10466406494379044 + 0.04884225130081177 + 0.0020551986526697874 = tensor(-0.0538, grad_fn=<MeanBackward0>)\n","0.109\n","-0.10070902109146118 + 0.041589345782995224 + 0.002055203774943948 = tensor(-0.0571, grad_fn=<MeanBackward0>)\n","0.097\n","-0.09613771736621857 + 0.052817922085523605 + 0.002053400268778205 = tensor(-0.0413, grad_fn=<MeanBackward0>)\n","0.107\n","-0.09894168376922607 + 0.03694312646985054 + 0.0020520570687949657 = tensor(-0.0599, grad_fn=<MeanBackward0>)\n","0.111\n","-0.09900129586458206 + 0.03873666375875473 + 0.0020572314970195293 = tensor(-0.0582, grad_fn=<MeanBackward0>)\n","0.104\n","-0.09725338220596313 + 0.04134145751595497 + 0.0020616822876036167 = tensor(-0.0539, grad_fn=<MeanBackward0>)\n","0.092\n","Train Epoch: 2 [10000/60000 (17%)]\tLoss: -0.053850\n","-0.09589812159538269 + 0.04440366104245186 + 0.002061740728095174 = tensor(-0.0494, grad_fn=<MeanBackward0>)\n","0.088\n","-0.09832310676574707 + 0.049828533083200455 + 0.002054866636171937 = tensor(-0.0464, grad_fn=<MeanBackward0>)\n","0.1\n","-0.09944145381450653 + 0.036011435091495514 + 0.002065769862383604 = tensor(-0.0614, grad_fn=<MeanBackward0>)\n","0.099\n","-0.10142580419778824 + 0.03739587590098381 + 0.0020573330111801624 = tensor(-0.0620, grad_fn=<MeanBackward0>)\n","0.122\n","-0.09288468211889267 + 0.040150173008441925 + 0.0020669035147875547 = tensor(-0.0507, grad_fn=<MeanBackward0>)\n","0.084\n","-0.09494789689779282 + 0.041932325810194016 + 0.0020653048995882273 = tensor(-0.0510, grad_fn=<MeanBackward0>)\n","0.096\n","-0.09784610569477081 + 0.03821153566241264 + 0.0020641025621443987 = tensor(-0.0576, grad_fn=<MeanBackward0>)\n","0.102\n","-0.10146691650152206 + 0.039156123995780945 + 0.0020658797584474087 = tensor(-0.0602, grad_fn=<MeanBackward0>)\n","0.111\n","-0.09657163172960281 + 0.05910572037100792 + 0.002069246955215931 = tensor(-0.0354, grad_fn=<MeanBackward0>)\n","0.083\n","-0.10781466215848923 + 0.05411116033792496 + 0.002066675340756774 = tensor(-0.0516, grad_fn=<MeanBackward0>)\n","0.115\n","Train Epoch: 2 [20000/60000 (33%)]\tLoss: -0.051637\n","-0.10070399194955826 + 0.04229380935430527 + 0.002068787580356002 = tensor(-0.0563, grad_fn=<MeanBackward0>)\n","0.099\n","-0.09678978472948074 + 0.04368501529097557 + 0.0020667859353125095 = tensor(-0.0510, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10365499556064606 + 0.03969993442296982 + 0.0020731757394969463 = tensor(-0.0619, grad_fn=<MeanBackward0>)\n","0.096\n","-0.09293302148580551 + 0.04570380598306656 + 0.002070198766887188 = tensor(-0.0452, grad_fn=<MeanBackward0>)\n","0.099\n","-0.11454759538173676 + 0.04654489457607269 + 0.002062712563201785 = tensor(-0.0659, grad_fn=<MeanBackward0>)\n","0.124\n","-0.09796357899904251 + 0.05387379601597786 + 0.0020718181040138006 = tensor(-0.0420, grad_fn=<MeanBackward0>)\n","0.093\n","-0.09228594601154327 + 0.0519193559885025 + 0.002064133295789361 = tensor(-0.0383, grad_fn=<MeanBackward0>)\n","0.077\n","-0.09531881660223007 + 0.04308928921818733 + 0.0020674364641308784 = tensor(-0.0502, grad_fn=<MeanBackward0>)\n","0.09\n","-0.09601129591464996 + 0.04002729803323746 + 0.0020687193609774113 = tensor(-0.0539, grad_fn=<MeanBackward0>)\n","0.093\n","-0.1029009222984314 + 0.04382709413766861 + 0.0020695568528026342 = tensor(-0.0570, grad_fn=<MeanBackward0>)\n","0.111\n","Train Epoch: 2 [30000/60000 (50%)]\tLoss: -0.057004\n","-0.09589600563049316 + 0.04454941302537918 + 0.0020697354339063168 = tensor(-0.0493, grad_fn=<MeanBackward0>)\n","0.101\n","-0.10539473593235016 + 0.036835432052612305 + 0.0020724672358483076 = tensor(-0.0665, grad_fn=<MeanBackward0>)\n","0.097\n","-0.10062701255083084 + 0.03366895765066147 + 0.0020645505283027887 = tensor(-0.0649, grad_fn=<MeanBackward0>)\n","0.098\n","-0.10502317547798157 + 0.050631117075681686 + 0.002068162662908435 = tensor(-0.0523, grad_fn=<MeanBackward0>)\n","0.102\n","-0.10180240869522095 + 0.044062692672014236 + 0.002067072782665491 = tensor(-0.0557, grad_fn=<MeanBackward0>)\n","0.107\n","-0.09535375982522964 + 0.03563887998461723 + 0.002062400570139289 = tensor(-0.0577, grad_fn=<MeanBackward0>)\n","0.089\n","-0.09944987297058105 + 0.042351238429546356 + 0.002068521687760949 = tensor(-0.0550, grad_fn=<MeanBackward0>)\n","0.107\n","-0.10048457235097885 + 0.04936385527253151 + 0.0020671728998422623 = tensor(-0.0491, grad_fn=<MeanBackward0>)\n","0.091\n","-0.10044581443071365 + 0.043640051037073135 + 0.0020714004058390856 = tensor(-0.0547, grad_fn=<MeanBackward0>)\n","0.111\n","-0.11220626533031464 + 0.046108026057481766 + 0.0020693298429250717 = tensor(-0.0640, grad_fn=<MeanBackward0>)\n","0.119\n","Train Epoch: 2 [40000/60000 (67%)]\tLoss: -0.064029\n","-0.0989539846777916 + 0.042852841317653656 + 0.002068818546831608 = tensor(-0.0540, grad_fn=<MeanBackward0>)\n","0.095\n","-0.10547105222940445 + 0.03987135738134384 + 0.0020735885482281446 = tensor(-0.0635, grad_fn=<MeanBackward0>)\n","0.104\n","-0.09451856464147568 + 0.0512273833155632 + 0.002071070484817028 = tensor(-0.0412, grad_fn=<MeanBackward0>)\n","0.089\n","-0.10522913932800293 + 0.0579896904528141 + 0.0020753901917487383 = tensor(-0.0452, grad_fn=<MeanBackward0>)\n","0.114\n","-0.10447743535041809 + 0.04092200845479965 + 0.0020738295279443264 = tensor(-0.0615, grad_fn=<MeanBackward0>)\n","0.104\n","-0.10280735045671463 + 0.03473389893770218 + 0.0020746863447129726 = tensor(-0.0660, grad_fn=<MeanBackward0>)\n","0.113\n","-0.10498633980751038 + 0.05977405235171318 + 0.0020700106397271156 = tensor(-0.0431, grad_fn=<MeanBackward0>)\n","0.115\n","-0.09570180624723434 + 0.05578457564115524 + 0.0020739235915243626 = tensor(-0.0378, grad_fn=<MeanBackward0>)\n","0.09\n","-0.09950476139783859 + 0.0339088998734951 + 0.00206999434158206 = tensor(-0.0635, grad_fn=<MeanBackward0>)\n","0.106\n","-0.10758296400308609 + 0.04370078817009926 + 0.0020733766723424196 = tensor(-0.0618, grad_fn=<MeanBackward0>)\n","0.109\n","Train Epoch: 2 [50000/60000 (83%)]\tLoss: -0.061809\n","-0.1057644709944725 + 0.045640185475349426 + 0.002069758018478751 = tensor(-0.0581, grad_fn=<MeanBackward0>)\n","0.107\n","-0.11298719793558121 + 0.05125993490219116 + 0.0020728905219584703 = tensor(-0.0597, grad_fn=<MeanBackward0>)\n","0.11\n","-0.10202620178461075 + 0.043648846447467804 + 0.0020706660579890013 = tensor(-0.0563, grad_fn=<MeanBackward0>)\n","0.112\n","-0.10169488191604614 + 0.05588375777006149 + 0.002065842505544424 = tensor(-0.0437, grad_fn=<MeanBackward0>)\n","0.11\n","-0.09609988331794739 + 0.04287081956863403 + 0.002073695883154869 = tensor(-0.0512, grad_fn=<MeanBackward0>)\n","0.103\n","-0.0997551754117012 + 0.05002966895699501 + 0.002070498652756214 = tensor(-0.0477, grad_fn=<MeanBackward0>)\n","0.094\n","-0.10117746144533157 + 0.049886930733919144 + 0.002077018842101097 = tensor(-0.0492, grad_fn=<MeanBackward0>)\n","0.09\n","-0.10232055932283401 + 0.04836031422019005 + 0.002071862807497382 = tensor(-0.0519, grad_fn=<MeanBackward0>)\n","0.098\n","-0.09854906052350998 + 0.03926277160644531 + 0.002073177369311452 = tensor(-0.0572, grad_fn=<MeanBackward0>)\n","0.09\n","-0.09773164987564087 + 0.04902748018503189 + 0.002074809744954109 = tensor(-0.0466, grad_fn=<MeanBackward0>)\n","-0.09540540724992752 + 0.040343593806028366 + 0.0020710125099867582 = tensor(-0.0530, grad_fn=<MeanBackward0>)\n","-0.09729021042585373 + 0.04399565979838371 + 0.0020691456738859415 = tensor(-0.0512, grad_fn=<MeanBackward0>)\n","-0.0959012359380722 + 0.036742646247148514 + 0.0020701182074844837 = tensor(-0.0571, grad_fn=<MeanBackward0>)\n","-0.09309355169534683 + 0.04446427896618843 + 0.002071758033707738 = tensor(-0.0466, grad_fn=<MeanBackward0>)\n","-0.09971600025892258 + 0.047512128949165344 + 0.00207743258215487 = tensor(-0.0501, grad_fn=<MeanBackward0>)\n","-0.09556125849485397 + 0.038525186479091644 + 0.0020757231395691633 = tensor(-0.0550, grad_fn=<MeanBackward0>)\n","-0.09938941895961761 + 0.0412861667573452 + 0.002076860051602125 = tensor(-0.0560, grad_fn=<MeanBackward0>)\n","-0.10319866985082626 + 0.037675339728593826 + 0.0020707077346742153 = tensor(-0.0635, grad_fn=<MeanBackward0>)\n","-0.09641237556934357 + 0.03005628101527691 + 0.002079020719975233 = tensor(-0.0643, grad_fn=<MeanBackward0>)\n","\n","Test set : Average loss : -0.0000, Accuracy: 0.9859999999999999/10000 (0%)\n","\n","-0.09448414295911789 + 0.04993817210197449 + 0.0020726723596453667 = tensor(-0.0425, grad_fn=<MeanBackward0>)\n","0.094\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: -0.042473\n","-0.09404829889535904 + 0.04548520967364311 + 0.002072033705189824 = tensor(-0.0465, grad_fn=<MeanBackward0>)\n","0.106\n","-0.10315298289060593 + 0.04825851321220398 + 0.002069863025099039 = tensor(-0.0528, grad_fn=<MeanBackward0>)\n","0.102\n","-0.09944317489862442 + 0.04666079953312874 + 0.002068244619295001 = tensor(-0.0507, grad_fn=<MeanBackward0>)\n","0.104\n","-0.1001327708363533 + 0.043488044291734695 + 0.0020646369084715843 = tensor(-0.0546, grad_fn=<MeanBackward0>)\n","0.097\n","-0.09790635854005814 + 0.04110739752650261 + 0.0020672946702688932 = tensor(-0.0547, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10475847125053406 + 0.049117568880319595 + 0.0020642338786274195 = tensor(-0.0536, grad_fn=<MeanBackward0>)\n","0.107\n","-0.09763452410697937 + 0.03894936293363571 + 0.0020618103444576263 = tensor(-0.0566, grad_fn=<MeanBackward0>)\n","0.092\n","-0.10366521030664444 + 0.05342312157154083 + 0.002061583334580064 = tensor(-0.0482, grad_fn=<MeanBackward0>)\n","0.105\n","-0.10005947202444077 + 0.039295900613069534 + 0.002061491832137108 = tensor(-0.0587, grad_fn=<MeanBackward0>)\n","0.106\n","-0.10372532904148102 + 0.030887417495250702 + 0.002064039697870612 = tensor(-0.0708, grad_fn=<MeanBackward0>)\n","0.095\n","Train Epoch: 3 [10000/60000 (17%)]\tLoss: -0.070774\n","-0.0973275825381279 + 0.03769564628601074 + 0.002063858089968562 = tensor(-0.0576, grad_fn=<MeanBackward0>)\n","0.09\n","-0.09429604560136795 + 0.04556787759065628 + 0.002068466739729047 = tensor(-0.0467, grad_fn=<MeanBackward0>)\n","0.096\n","-0.10333262383937836 + 0.04073306918144226 + 0.0020639861468225718 = tensor(-0.0605, grad_fn=<MeanBackward0>)\n","0.107\n","-0.09331447631120682 + 0.041239961981773376 + 0.0020593595691025257 = tensor(-0.0500, grad_fn=<MeanBackward0>)\n","0.091\n","-0.09152518957853317 + 0.04464567080140114 + 0.0020573746878653765 = tensor(-0.0448, grad_fn=<MeanBackward0>)\n","0.083\n","-0.10082528740167618 + 0.036822814494371414 + 0.0020568198524415493 = tensor(-0.0619, grad_fn=<MeanBackward0>)\n","0.102\n","-0.10661067068576813 + 0.03998206928372383 + 0.0020552577916532755 = tensor(-0.0646, grad_fn=<MeanBackward0>)\n","0.106\n","-0.09775698184967041 + 0.04677320271730423 + 0.002057488542050123 = tensor(-0.0489, grad_fn=<MeanBackward0>)\n","0.107\n","-0.10663966834545135 + 0.03495399281382561 + 0.0020539008546620607 = tensor(-0.0696, grad_fn=<MeanBackward0>)\n","0.108\n","-0.11106507480144501 + 0.04948943480849266 + 0.0020517620723694563 = tensor(-0.0595, grad_fn=<MeanBackward0>)\n","0.116\n","Train Epoch: 3 [20000/60000 (33%)]\tLoss: -0.059524\n","-0.10489382594823837 + 0.03074166551232338 + 0.0020577574614435434 = tensor(-0.0721, grad_fn=<MeanBackward0>)\n","0.091\n","-0.10388106107711792 + 0.04841272160410881 + 0.0020517497323453426 = tensor(-0.0534, grad_fn=<MeanBackward0>)\n","0.104\n","-0.10762367397546768 + 0.045321352779865265 + 0.0020495194476097822 = tensor(-0.0603, grad_fn=<MeanBackward0>)\n","0.096\n","-0.09964599460363388 + 0.03988247737288475 + 0.0020525609143078327 = tensor(-0.0577, grad_fn=<MeanBackward0>)\n","0.107\n","-0.0932069644331932 + 0.03846778720617294 + 0.002057597739621997 = tensor(-0.0527, grad_fn=<MeanBackward0>)\n","0.076\n","-0.09279119968414307 + 0.03924217075109482 + 0.002056115074083209 = tensor(-0.0515, grad_fn=<MeanBackward0>)\n","0.081\n","-0.09726855158805847 + 0.03351728990674019 + 0.002051485003903508 = tensor(-0.0617, grad_fn=<MeanBackward0>)\n","0.097\n","-0.0958486795425415 + 0.03747055307030678 + 0.0020523485727608204 = tensor(-0.0563, grad_fn=<MeanBackward0>)\n","0.091\n","-0.10996851325035095 + 0.04625000059604645 + 0.002051963936537504 = tensor(-0.0617, grad_fn=<MeanBackward0>)\n","0.114\n","-0.10711796581745148 + 0.037575364112854004 + 0.0020564808510243893 = tensor(-0.0675, grad_fn=<MeanBackward0>)\n","0.105\n","Train Epoch: 3 [30000/60000 (50%)]\tLoss: -0.067486\n","-0.09240256994962692 + 0.04357383772730827 + 0.0020548542961478233 = tensor(-0.0468, grad_fn=<MeanBackward0>)\n","0.092\n","-0.09146486222743988 + 0.04547378793358803 + 0.0020552293863147497 = tensor(-0.0439, grad_fn=<MeanBackward0>)\n","0.087\n","-0.10102944821119308 + 0.03811585530638695 + 0.002056367928162217 = tensor(-0.0609, grad_fn=<MeanBackward0>)\n","0.104\n","-0.10594344139099121 + 0.04641539976000786 + 0.002053813776001334 = tensor(-0.0575, grad_fn=<MeanBackward0>)\n","0.104\n","-0.0995391234755516 + 0.04342202469706535 + 0.002051743445917964 = tensor(-0.0541, grad_fn=<MeanBackward0>)\n","0.107\n","-0.09870146960020065 + 0.041506823152303696 + 0.002055190270766616 = tensor(-0.0551, grad_fn=<MeanBackward0>)\n","0.113\n","-0.10412350296974182 + 0.03966521471738815 + 0.0020568999461829662 = tensor(-0.0624, grad_fn=<MeanBackward0>)\n","0.108\n","-0.09710376709699631 + 0.04488484188914299 + 0.0020573243964463472 = tensor(-0.0502, grad_fn=<MeanBackward0>)\n","0.096\n","-0.09472563117742538 + 0.03624749928712845 + 0.0020608508493751287 = tensor(-0.0564, grad_fn=<MeanBackward0>)\n","0.1\n","-0.09823586046695709 + 0.04099123924970627 + 0.002058308105915785 = tensor(-0.0552, grad_fn=<MeanBackward0>)\n","0.102\n","Train Epoch: 3 [40000/60000 (67%)]\tLoss: -0.055186\n","-0.10357813537120819 + 0.0403115414083004 + 0.00205934583209455 = tensor(-0.0612, grad_fn=<MeanBackward0>)\n","0.106\n","-0.11271215230226517 + 0.04502062872052193 + 0.002056614262983203 = tensor(-0.0656, grad_fn=<MeanBackward0>)\n","0.124\n","-0.09476678818464279 + 0.0615493506193161 + 0.0020556331146508455 = tensor(-0.0312, grad_fn=<MeanBackward0>)\n","0.095\n","-0.09655239433050156 + 0.043094806373119354 + 0.0020660809241235256 = tensor(-0.0514, grad_fn=<MeanBackward0>)\n","0.091\n","-0.09856869280338287 + 0.028706330806016922 + 0.0020583446603268385 = tensor(-0.0678, grad_fn=<MeanBackward0>)\n","0.11\n","-0.09242536127567291 + 0.03159203752875328 + 0.0020610583014786243 = tensor(-0.0588, grad_fn=<MeanBackward0>)\n","0.097\n","-0.10210774838924408 + 0.04006745666265488 + 0.002059572609141469 = tensor(-0.0600, grad_fn=<MeanBackward0>)\n","0.11\n","-0.10115817189216614 + 0.052648305892944336 + 0.0020573846995830536 = tensor(-0.0465, grad_fn=<MeanBackward0>)\n","0.108\n","-0.10874111205339432 + 0.047259654849767685 + 0.0020556652452796698 = tensor(-0.0594, grad_fn=<MeanBackward0>)\n","0.096\n","-0.10153131186962128 + 0.04490591958165169 + 0.0020591234788298607 = tensor(-0.0546, grad_fn=<MeanBackward0>)\n","0.093\n","Train Epoch: 3 [50000/60000 (83%)]\tLoss: -0.054566\n","-0.09616004675626755 + 0.04372473061084747 + 0.002054909011349082 = tensor(-0.0504, grad_fn=<MeanBackward0>)\n","0.098\n","-0.10335574299097061 + 0.030460011214017868 + 0.0020552915520966053 = tensor(-0.0708, grad_fn=<MeanBackward0>)\n","0.102\n","-0.11547870934009552 + 0.04196484386920929 + 0.002057644771412015 = tensor(-0.0715, grad_fn=<MeanBackward0>)\n","0.127\n","-0.10557354986667633 + 0.03865903988480568 + 0.002055589109659195 = tensor(-0.0649, grad_fn=<MeanBackward0>)\n","0.105\n","-0.09393082559108734 + 0.04331498593091965 + 0.0020583427976816893 = tensor(-0.0486, grad_fn=<MeanBackward0>)\n","0.095\n","-0.09182550013065338 + 0.03496745601296425 + 0.0020590261556208134 = tensor(-0.0548, grad_fn=<MeanBackward0>)\n","0.094\n","-0.09872165322303772 + 0.04093635454773903 + 0.0020576214883476496 = tensor(-0.0557, grad_fn=<MeanBackward0>)\n","0.093\n","-0.10229114443063736 + 0.041049275547266006 + 0.0020650560036301613 = tensor(-0.0592, grad_fn=<MeanBackward0>)\n","0.112\n","-0.09958162903785706 + 0.03305886313319206 + 0.0020658571738749743 = tensor(-0.0645, grad_fn=<MeanBackward0>)\n","0.106\n","-0.09448348730802536 + 0.04253186285495758 + 0.002065730979666114 = tensor(-0.0499, grad_fn=<MeanBackward0>)\n","-0.09838507324457169 + 0.04567759484052658 + 0.002064619679003954 = tensor(-0.0506, grad_fn=<MeanBackward0>)\n","-0.0989832803606987 + 0.03687921538949013 + 0.002062048763036728 = tensor(-0.0600, grad_fn=<MeanBackward0>)\n","-0.10137239098548889 + 0.04336774721741676 + 0.002061990089714527 = tensor(-0.0559, grad_fn=<MeanBackward0>)\n","-0.10507994890213013 + 0.03996282070875168 + 0.0020599118433892727 = tensor(-0.0631, grad_fn=<MeanBackward0>)\n","-0.096453458070755 + 0.04583418369293213 + 0.002058197045698762 = tensor(-0.0486, grad_fn=<MeanBackward0>)\n","-0.10545458644628525 + 0.035941991955041885 + 0.002061588456854224 = tensor(-0.0675, grad_fn=<MeanBackward0>)\n","-0.1049005538225174 + 0.04462543874979019 + 0.0020663754548877478 = tensor(-0.0582, grad_fn=<MeanBackward0>)\n","-0.09599844366312027 + 0.03266539424657822 + 0.002066838787868619 = tensor(-0.0613, grad_fn=<MeanBackward0>)\n","-0.09254229813814163 + 0.03026387095451355 + 0.002068388042971492 = tensor(-0.0602, grad_fn=<MeanBackward0>)\n","\n","Test set : Average loss : -0.0000, Accuracy: 0.9889999999999999/10000 (0%)\n","\n","-0.10099394619464874 + 0.04076218977570534 + 0.0020670907106250525 = tensor(-0.0582, grad_fn=<MeanBackward0>)\n","0.091\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: -0.058165\n","-0.1018291562795639 + 0.04137084260582924 + 0.00205818610265851 = tensor(-0.0584, grad_fn=<MeanBackward0>)\n","0.097\n","-0.10442593693733215 + 0.039667461067438126 + 0.002057646168395877 = tensor(-0.0627, grad_fn=<MeanBackward0>)\n","0.091\n","-0.1044904813170433 + 0.04186752811074257 + 0.0020597302354872227 = tensor(-0.0606, grad_fn=<MeanBackward0>)\n","0.09\n","-0.10269039869308472 + 0.04652929678559303 + 0.0020610594656318426 = tensor(-0.0541, grad_fn=<MeanBackward0>)\n","0.098\n","-0.09789104759693146 + 0.041470717638731 + 0.002058528596535325 = tensor(-0.0544, grad_fn=<MeanBackward0>)\n","0.094\n","-0.09728843718767166 + 0.03723417595028877 + 0.0020671349484473467 = tensor(-0.0580, grad_fn=<MeanBackward0>)\n","0.093\n","-0.09725380688905716 + 0.03441094234585762 + 0.0020640837028622627 = tensor(-0.0608, grad_fn=<MeanBackward0>)\n","0.094\n","-0.10411442816257477 + 0.04218820109963417 + 0.002063692780211568 = tensor(-0.0599, grad_fn=<MeanBackward0>)\n","0.123\n","-0.09526610374450684 + 0.040388163179159164 + 0.002063135849311948 = tensor(-0.0528, grad_fn=<MeanBackward0>)\n","0.097\n","-0.10054214298725128 + 0.04467325657606125 + 0.00205636746250093 = tensor(-0.0538, grad_fn=<MeanBackward0>)\n","0.098\n","Train Epoch: 4 [10000/60000 (17%)]\tLoss: -0.053813\n","-0.1014520451426506 + 0.045323267579078674 + 0.0020676213316619396 = tensor(-0.0541, grad_fn=<MeanBackward0>)\n","0.101\n","-0.10530909895896912 + 0.03917001187801361 + 0.002054413314908743 = tensor(-0.0641, grad_fn=<MeanBackward0>)\n","0.102\n","-0.09736302495002747 + 0.0398963987827301 + 0.002060074359178543 = tensor(-0.0554, grad_fn=<MeanBackward0>)\n","0.105\n","-0.09882351756095886 + 0.035686932504177094 + 0.0020620825234800577 = tensor(-0.0611, grad_fn=<MeanBackward0>)\n","0.098\n","-0.10487782210111618 + 0.030163543298840523 + 0.0020621432922780514 = tensor(-0.0727, grad_fn=<MeanBackward0>)\n","0.111\n","-0.10443167388439178 + 0.03255874291062355 + 0.00206177169457078 = tensor(-0.0698, grad_fn=<MeanBackward0>)\n","0.095\n","-0.09997599571943283 + 0.03309773653745651 + 0.002060818951576948 = tensor(-0.0648, grad_fn=<MeanBackward0>)\n","0.098\n","-0.102138951420784 + 0.03806844353675842 + 0.002063858089968562 = tensor(-0.0620, grad_fn=<MeanBackward0>)\n","0.117\n","-0.1071009486913681 + 0.031858496367931366 + 0.0020649165380746126 = tensor(-0.0732, grad_fn=<MeanBackward0>)\n","0.102\n","-0.0926804393529892 + 0.04572059214115143 + 0.0020699144806712866 = tensor(-0.0449, grad_fn=<MeanBackward0>)\n","0.087\n","Train Epoch: 4 [20000/60000 (33%)]\tLoss: -0.044890\n","-0.10407090932130814 + 0.053385213017463684 + 0.002069744747132063 = tensor(-0.0486, grad_fn=<MeanBackward0>)\n","0.113\n","-0.09929309040307999 + 0.033711690455675125 + 0.0020666876807808876 = tensor(-0.0635, grad_fn=<MeanBackward0>)\n","0.096\n","-0.09634075313806534 + 0.05014583468437195 + 0.0020727007649838924 = tensor(-0.0441, grad_fn=<MeanBackward0>)\n","0.099\n","-0.09596028178930283 + 0.042533744126558304 + 0.002076301956549287 = tensor(-0.0514, grad_fn=<MeanBackward0>)\n","0.095\n","-0.10123220831155777 + 0.04954241216182709 + 0.002073254669085145 = tensor(-0.0496, grad_fn=<MeanBackward0>)\n","0.102\n","-0.10005221515893936 + 0.048611171543598175 + 0.00206934567540884 = tensor(-0.0494, grad_fn=<MeanBackward0>)\n","0.103\n","-0.09966498613357544 + 0.03916718810796738 + 0.0020694073755294085 = tensor(-0.0584, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10402838885784149 + 0.04320526868104935 + 0.00206994847394526 = tensor(-0.0588, grad_fn=<MeanBackward0>)\n","0.11\n","-0.10320191830396652 + 0.04007742926478386 + 0.0020643204916268587 = tensor(-0.0611, grad_fn=<MeanBackward0>)\n","0.104\n","-0.1039123386144638 + 0.036123648285865784 + 0.002068601083010435 = tensor(-0.0657, grad_fn=<MeanBackward0>)\n","0.1\n","Train Epoch: 4 [30000/60000 (50%)]\tLoss: -0.065720\n","-0.10795946419239044 + 0.042265381664037704 + 0.002067241817712784 = tensor(-0.0636, grad_fn=<MeanBackward0>)\n","0.112\n","-0.1017899438738823 + 0.040675487369298935 + 0.0020696446299552917 = tensor(-0.0590, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10116127133369446 + 0.03777085617184639 + 0.002068817615509033 = tensor(-0.0613, grad_fn=<MeanBackward0>)\n","0.095\n","-0.0904572382569313 + 0.029187895357608795 + 0.0020623947493731976 = tensor(-0.0592, grad_fn=<MeanBackward0>)\n","0.079\n","-0.09954207390546799 + 0.04061300680041313 + 0.0020659498404711485 = tensor(-0.0569, grad_fn=<MeanBackward0>)\n","0.087\n","-0.1011248528957367 + 0.040005408227443695 + 0.002063724212348461 = tensor(-0.0591, grad_fn=<MeanBackward0>)\n","0.105\n","-0.09536147117614746 + 0.03428215906023979 + 0.0020687533542513847 = tensor(-0.0590, grad_fn=<MeanBackward0>)\n","0.082\n","-0.09327671676874161 + 0.04851922392845154 + 0.0020640743896365166 = tensor(-0.0427, grad_fn=<MeanBackward0>)\n","0.1\n","-0.09828965365886688 + 0.04163571819663048 + 0.0020675184205174446 = tensor(-0.0546, grad_fn=<MeanBackward0>)\n","0.088\n","-0.10650495439767838 + 0.04233231395483017 + 0.002062142128124833 = tensor(-0.0621, grad_fn=<MeanBackward0>)\n","0.111\n","Train Epoch: 4 [40000/60000 (67%)]\tLoss: -0.062110\n","-0.09028658270835876 + 0.03442526236176491 + 0.0020586431492120028 = tensor(-0.0538, grad_fn=<MeanBackward0>)\n","0.082\n","-0.09708151966333389 + 0.04256536811590195 + 0.0020603323355317116 = tensor(-0.0525, grad_fn=<MeanBackward0>)\n","0.088\n","-0.08889289945363998 + 0.04133646562695503 + 0.002060599159449339 = tensor(-0.0455, grad_fn=<MeanBackward0>)\n","0.086\n","-0.09754199534654617 + 0.0382637158036232 + 0.002059731399640441 = tensor(-0.0572, grad_fn=<MeanBackward0>)\n","0.101\n","-0.09545455873012543 + 0.050616491585969925 + 0.002057173987850547 = tensor(-0.0428, grad_fn=<MeanBackward0>)\n","0.086\n","-0.10355433821678162 + 0.03409971669316292 + 0.0020584152080118656 = tensor(-0.0674, grad_fn=<MeanBackward0>)\n","0.102\n","-0.09628979861736298 + 0.04178832471370697 + 0.0020610615611076355 = tensor(-0.0524, grad_fn=<MeanBackward0>)\n","0.093\n","-0.10446986556053162 + 0.03816678375005722 + 0.0020540920086205006 = tensor(-0.0642, grad_fn=<MeanBackward0>)\n","0.112\n","-0.10113583505153656 + 0.044112127274274826 + 0.002057022415101528 = tensor(-0.0550, grad_fn=<MeanBackward0>)\n","0.101\n","-0.09923054277896881 + 0.05119623243808746 + 0.0020572501234710217 = tensor(-0.0460, grad_fn=<MeanBackward0>)\n","0.095\n","Train Epoch: 4 [50000/60000 (83%)]\tLoss: -0.045977\n","-0.10504449158906937 + 0.03953860327601433 + 0.002051783725619316 = tensor(-0.0635, grad_fn=<MeanBackward0>)\n","0.108\n","-0.10417206585407257 + 0.03317529708147049 + 0.0020533516071736813 = tensor(-0.0689, grad_fn=<MeanBackward0>)\n","0.107\n","-0.10212746262550354 + 0.03264464810490608 + 0.002058646874502301 = tensor(-0.0674, grad_fn=<MeanBackward0>)\n","0.099\n","-0.09993210434913635 + 0.039690885692834854 + 0.002057383768260479 = tensor(-0.0582, grad_fn=<MeanBackward0>)\n","0.112\n","-0.09710650891065598 + 0.04154849052429199 + 0.0020525625441223383 = tensor(-0.0535, grad_fn=<MeanBackward0>)\n","0.098\n","-0.10293619334697723 + 0.04285934939980507 + 0.0020587577018886805 = tensor(-0.0580, grad_fn=<MeanBackward0>)\n","0.101\n","-0.10175634920597076 + 0.043383460491895676 + 0.002054720651358366 = tensor(-0.0563, grad_fn=<MeanBackward0>)\n","0.1\n","-0.09804243594408035 + 0.041443534195423126 + 0.0020533213391900063 = tensor(-0.0545, grad_fn=<MeanBackward0>)\n","0.095\n","-0.0984174832701683 + 0.036623515188694 + 0.0020514987409114838 = tensor(-0.0597, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10457385331392288 + 0.04087240248918533 + 0.002056811237707734 = tensor(-0.0616, grad_fn=<MeanBackward0>)\n","-0.09907706081867218 + 0.03759785741567612 + 0.002058384707197547 = tensor(-0.0594, grad_fn=<MeanBackward0>)\n","-0.09635093063116074 + 0.04081326723098755 + 0.002053932286798954 = tensor(-0.0535, grad_fn=<MeanBackward0>)\n","-0.09928468614816666 + 0.04396318644285202 + 0.0020629363134503365 = tensor(-0.0533, grad_fn=<MeanBackward0>)\n","-0.10578660666942596 + 0.04096386954188347 + 0.002056497149169445 = tensor(-0.0628, grad_fn=<MeanBackward0>)\n","-0.10700956732034683 + 0.05320001021027565 + 0.0020646145567297935 = tensor(-0.0517, grad_fn=<MeanBackward0>)\n","-0.10128656774759293 + 0.0459803082048893 + 0.0020608161576092243 = tensor(-0.0532, grad_fn=<MeanBackward0>)\n","-0.10572215914726257 + 0.04251434653997421 + 0.0020561697892844677 = tensor(-0.0612, grad_fn=<MeanBackward0>)\n","-0.1036931499838829 + 0.03563104197382927 + 0.0020633977837860584 = tensor(-0.0660, grad_fn=<MeanBackward0>)\n","-0.10290838032960892 + 0.033710334450006485 + 0.002059289952740073 = tensor(-0.0671, grad_fn=<MeanBackward0>)\n","\n","Test set : Average loss : -0.0000, Accuracy: 1.059/10000 (0%)\n","\n","-0.09937631338834763 + 0.0464184544980526 + 0.0020525448489934206 = tensor(-0.0509, grad_fn=<MeanBackward0>)\n","0.089\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: -0.050905\n","-0.097242571413517 + 0.03872616961598396 + 0.0020593362860381603 = tensor(-0.0565, grad_fn=<MeanBackward0>)\n","0.113\n","-0.09792405366897583 + 0.037438660860061646 + 0.0020547097083181143 = tensor(-0.0584, grad_fn=<MeanBackward0>)\n","0.094\n","-0.09894631057977676 + 0.04254409670829773 + 0.0020512286573648453 = tensor(-0.0544, grad_fn=<MeanBackward0>)\n","0.098\n","-0.08996468782424927 + 0.05350242182612419 + 0.002054790733382106 = tensor(-0.0344, grad_fn=<MeanBackward0>)\n","0.081\n","-0.09688477963209152 + 0.028501136228442192 + 0.00205037510022521 = tensor(-0.0663, grad_fn=<MeanBackward0>)\n","0.092\n","-0.10625603795051575 + 0.036355018615722656 + 0.002056505996733904 = tensor(-0.0678, grad_fn=<MeanBackward0>)\n","0.12\n","-0.08890533447265625 + 0.045088544487953186 + 0.002058723708614707 = tensor(-0.0418, grad_fn=<MeanBackward0>)\n","0.083\n","-0.10140306502580643 + 0.03746157884597778 + 0.002056407742202282 = tensor(-0.0619, grad_fn=<MeanBackward0>)\n","0.1\n","-0.09387035667896271 + 0.048515886068344116 + 0.0020529909525066614 = tensor(-0.0433, grad_fn=<MeanBackward0>)\n","0.101\n","-0.10104392468929291 + 0.03793336823582649 + 0.0020543294958770275 = tensor(-0.0611, grad_fn=<MeanBackward0>)\n","0.1\n","Train Epoch: 5 [10000/60000 (17%)]\tLoss: -0.061056\n","-0.09417476505041122 + 0.0486871674656868 + 0.002058537444099784 = tensor(-0.0434, grad_fn=<MeanBackward0>)\n","0.088\n","-0.1106402650475502 + 0.0424637496471405 + 0.002056949771940708 = tensor(-0.0661, grad_fn=<MeanBackward0>)\n","0.106\n","-0.09848691523075104 + 0.03937441110610962 + 0.0020478512160480022 = tensor(-0.0571, grad_fn=<MeanBackward0>)\n","0.096\n","-0.09432493150234222 + 0.03862226754426956 + 0.0020492440089583397 = tensor(-0.0537, grad_fn=<MeanBackward0>)\n","0.082\n","-0.10342084616422653 + 0.04095808416604996 + 0.0020494251511991024 = tensor(-0.0604, grad_fn=<MeanBackward0>)\n","0.105\n","-0.10276851803064346 + 0.04183795675635338 + 0.002049326431006193 = tensor(-0.0589, grad_fn=<MeanBackward0>)\n","0.103\n","-0.09663551300764084 + 0.03441612794995308 + 0.002049281494691968 = tensor(-0.0602, grad_fn=<MeanBackward0>)\n","0.099\n","-0.10340747982263565 + 0.04126771539449692 + 0.002045111730694771 = tensor(-0.0601, grad_fn=<MeanBackward0>)\n","0.107\n","-0.10334233939647675 + 0.034377504140138626 + 0.0020487650763243437 = tensor(-0.0669, grad_fn=<MeanBackward0>)\n","0.103\n","-0.10085504502058029 + 0.04149341955780983 + 0.0020520424004644156 = tensor(-0.0573, grad_fn=<MeanBackward0>)\n","0.107\n","Train Epoch: 5 [20000/60000 (33%)]\tLoss: -0.057310\n","-0.10147617757320404 + 0.0451224260032177 + 0.002046364126726985 = tensor(-0.0543, grad_fn=<MeanBackward0>)\n","0.109\n","-0.09818913042545319 + 0.032840657979249954 + 0.002051240298897028 = tensor(-0.0633, grad_fn=<MeanBackward0>)\n","0.1\n","-0.09562075883150101 + 0.042802609503269196 + 0.002050838666036725 = tensor(-0.0508, grad_fn=<MeanBackward0>)\n","0.086\n","-0.10450419038534164 + 0.04464022070169449 + 0.0020470721647143364 = tensor(-0.0578, grad_fn=<MeanBackward0>)\n","0.112\n","-0.10560023039579391 + 0.03664498031139374 + 0.002052247291430831 = tensor(-0.0669, grad_fn=<MeanBackward0>)\n","0.1\n","-0.09979109466075897 + 0.042677219957113266 + 0.0020492682233452797 = tensor(-0.0551, grad_fn=<MeanBackward0>)\n","0.097\n","-0.10010261833667755 + 0.04567597806453705 + 0.0020506258588284254 = tensor(-0.0524, grad_fn=<MeanBackward0>)\n","0.097\n","-0.09693075716495514 + 0.04038558900356293 + 0.002047817688435316 = tensor(-0.0545, grad_fn=<MeanBackward0>)\n","0.108\n","-0.10231275111436844 + 0.04578860104084015 + 0.0020523846615105867 = tensor(-0.0545, grad_fn=<MeanBackward0>)\n","0.105\n","-0.10263334214687347 + 0.03790076822042465 + 0.0020502635743469 = tensor(-0.0627, grad_fn=<MeanBackward0>)\n","0.111\n","Train Epoch: 5 [30000/60000 (50%)]\tLoss: -0.062682\n","-0.0932944044470787 + 0.0395159050822258 + 0.0020579874981194735 = tensor(-0.0517, grad_fn=<MeanBackward0>)\n","0.09\n","-0.1098322793841362 + 0.0431489571928978 + 0.0020516812801361084 = tensor(-0.0646, grad_fn=<MeanBackward0>)\n","0.114\n","-0.10381362587213516 + 0.0404127836227417 + 0.002056777011603117 = tensor(-0.0613, grad_fn=<MeanBackward0>)\n","0.109\n","-0.10053962469100952 + 0.047501612454652786 + 0.0020537867676466703 = tensor(-0.0510, grad_fn=<MeanBackward0>)\n","0.102\n","-0.1002495214343071 + 0.03576117381453514 + 0.002049659378826618 = tensor(-0.0624, grad_fn=<MeanBackward0>)\n","0.108\n","-0.10430824756622314 + 0.03682226315140724 + 0.0020536950323730707 = tensor(-0.0654, grad_fn=<MeanBackward0>)\n","0.117\n","-0.10206739604473114 + 0.04136612266302109 + 0.0020518838427960873 = tensor(-0.0586, grad_fn=<MeanBackward0>)\n","0.099\n","-0.09719682484865189 + 0.030578255653381348 + 0.0020518312230706215 = tensor(-0.0646, grad_fn=<MeanBackward0>)\n","0.094\n","-0.09343022108078003 + 0.03903008997440338 + 0.002045264234766364 = tensor(-0.0524, grad_fn=<MeanBackward0>)\n","0.093\n","-0.09947796911001205 + 0.03957008570432663 + 0.002053134609013796 = tensor(-0.0579, grad_fn=<MeanBackward0>)\n","0.096\n","Train Epoch: 5 [40000/60000 (67%)]\tLoss: -0.057855\n","-0.0940447449684143 + 0.031719282269477844 + 0.0020534987561404705 = tensor(-0.0603, grad_fn=<MeanBackward0>)\n","0.099\n","-0.10611417144536972 + 0.026072682812809944 + 0.002048380207270384 = tensor(-0.0780, grad_fn=<MeanBackward0>)\n","0.113\n","-0.09584642946720123 + 0.025852767750620842 + 0.0020485587883740664 = tensor(-0.0679, grad_fn=<MeanBackward0>)\n","0.094\n","-0.09962109476327896 + 0.04016325995326042 + 0.002054366283118725 = tensor(-0.0574, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10033413022756577 + 0.0367700532078743 + 0.0020500188693404198 = tensor(-0.0615, grad_fn=<MeanBackward0>)\n","0.093\n","-0.10425738990306854 + 0.04187167435884476 + 0.0020501306280493736 = tensor(-0.0603, grad_fn=<MeanBackward0>)\n","0.102\n","-0.10024014860391617 + 0.03440077602863312 + 0.0020534484647214413 = tensor(-0.0638, grad_fn=<MeanBackward0>)\n","0.104\n","-0.0999135971069336 + 0.037629738450050354 + 0.002055531134828925 = tensor(-0.0602, grad_fn=<MeanBackward0>)\n","0.095\n","-0.10175856202840805 + 0.04876140505075455 + 0.0020531336776912212 = tensor(-0.0509, grad_fn=<MeanBackward0>)\n","0.103\n","-0.10221975296735764 + 0.03579597547650337 + 0.0020525080617517233 = tensor(-0.0644, grad_fn=<MeanBackward0>)\n","0.104\n","Train Epoch: 5 [50000/60000 (83%)]\tLoss: -0.064371\n","-0.09947052597999573 + 0.04166913032531738 + 0.002053892472758889 = tensor(-0.0557, grad_fn=<MeanBackward0>)\n","0.104\n","-0.09875494986772537 + 0.04246331751346588 + 0.0020524179562926292 = tensor(-0.0542, grad_fn=<MeanBackward0>)\n","0.095\n","-0.10018984228372574 + 0.03541995957493782 + 0.0020507373847067356 = tensor(-0.0627, grad_fn=<MeanBackward0>)\n","0.094\n","-0.09875334054231644 + 0.04031636565923691 + 0.002054384211078286 = tensor(-0.0564, grad_fn=<MeanBackward0>)\n","0.094\n","-0.09756214171648026 + 0.04410354048013687 + 0.002053830772638321 = tensor(-0.0514, grad_fn=<MeanBackward0>)\n","0.097\n","-0.0977257564663887 + 0.037003885954618454 + 0.002054389100521803 = tensor(-0.0587, grad_fn=<MeanBackward0>)\n","0.095\n","-0.10314294695854187 + 0.042616233229637146 + 0.0020524656865745783 = tensor(-0.0585, grad_fn=<MeanBackward0>)\n","0.107\n","-0.10593225806951523 + 0.03034150041639805 + 0.002050831215456128 = tensor(-0.0735, grad_fn=<MeanBackward0>)\n","0.112\n","-0.10220441222190857 + 0.041938602924346924 + 0.0020521192345768213 = tensor(-0.0582, grad_fn=<MeanBackward0>)\n","0.108\n","-0.10233193635940552 + 0.04093681275844574 + 0.002057740930467844 = tensor(-0.0593, grad_fn=<MeanBackward0>)\n","-0.10202326625585556 + 0.034165915101766586 + 0.002052257303148508 = tensor(-0.0658, grad_fn=<MeanBackward0>)\n","-0.10197485983371735 + 0.03788885846734047 + 0.0020503555424511433 = tensor(-0.0620, grad_fn=<MeanBackward0>)\n","-0.10486366599798203 + 0.030369913205504417 + 0.0020562605932354927 = tensor(-0.0724, grad_fn=<MeanBackward0>)\n","-0.09347739815711975 + 0.04051021486520767 + 0.002056092955172062 = tensor(-0.0509, grad_fn=<MeanBackward0>)\n","-0.09885475039482117 + 0.043886806815862656 + 0.002053341595456004 = tensor(-0.0529, grad_fn=<MeanBackward0>)\n","-0.09602378308773041 + 0.03543009236454964 + 0.002059399848803878 = tensor(-0.0585, grad_fn=<MeanBackward0>)\n","-0.10157857835292816 + 0.044643424451351166 + 0.00205913744866848 = tensor(-0.0549, grad_fn=<MeanBackward0>)\n","-0.09634595364332199 + 0.032364197075366974 + 0.002053116215392947 = tensor(-0.0619, grad_fn=<MeanBackward0>)\n","-0.09934904426336288 + 0.034909237176179886 + 0.002056804718449712 = tensor(-0.0624, grad_fn=<MeanBackward0>)\n","\n","Test set : Average loss : -0.0000, Accuracy: 0.9689999999999999/10000 (0%)\n","\n","-0.09905561059713364 + 0.050713103264570236 + 0.0020542864222079515 = tensor(-0.0463, grad_fn=<MeanBackward0>)\n","0.104\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: -0.046288\n","-0.10735524445772171 + 0.0386074036359787 + 0.0020518756937235594 = tensor(-0.0667, grad_fn=<MeanBackward0>)\n","0.12\n","-0.10430867224931717 + 0.041504424065351486 + 0.0020556256640702486 = tensor(-0.0607, grad_fn=<MeanBackward0>)\n","0.104\n","-0.08890601992607117 + 0.04678056016564369 + 0.002050393959507346 = tensor(-0.0401, grad_fn=<MeanBackward0>)\n","0.087\n","-0.0977153480052948 + 0.04047029837965965 + 0.0020490142051130533 = tensor(-0.0552, grad_fn=<MeanBackward0>)\n","0.093\n","-0.09747222065925598 + 0.04285725951194763 + 0.002046933164820075 = tensor(-0.0526, grad_fn=<MeanBackward0>)\n","0.095\n","-0.10155350714921951 + 0.03265853971242905 + 0.0020466321147978306 = tensor(-0.0668, grad_fn=<MeanBackward0>)\n","0.101\n","-0.09662855416536331 + 0.03620607405900955 + 0.002049683593213558 = tensor(-0.0584, grad_fn=<MeanBackward0>)\n","0.088\n","-0.09727279096841812 + 0.03051677718758583 + 0.0020460246596485376 = tensor(-0.0647, grad_fn=<MeanBackward0>)\n","0.088\n","-0.10425926744937897 + 0.040842246264219284 + 0.002044202759861946 = tensor(-0.0614, grad_fn=<MeanBackward0>)\n","0.107\n","-0.10194330662488937 + 0.049435872584581375 + 0.0020456823986023664 = tensor(-0.0505, grad_fn=<MeanBackward0>)\n","0.109\n","Train Epoch: 6 [10000/60000 (17%)]\tLoss: -0.050462\n","-0.09738467633724213 + 0.040388572961091995 + 0.002043254440650344 = tensor(-0.0550, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10366260260343552 + 0.04281376674771309 + 0.002043645828962326 = tensor(-0.0588, grad_fn=<MeanBackward0>)\n","0.11\n","-0.09541711211204529 + 0.03740913048386574 + 0.0020457045175135136 = tensor(-0.0560, grad_fn=<MeanBackward0>)\n","0.09\n","-0.10839558392763138 + 0.03206890821456909 + 0.002041380386799574 = tensor(-0.0743, grad_fn=<MeanBackward0>)\n","0.132\n","-0.09683693945407867 + 0.031024456024169922 + 0.0020351666025817394 = tensor(-0.0638, grad_fn=<MeanBackward0>)\n","0.098\n","-0.10044939815998077 + 0.035189438611269 + 0.0020444204565137625 = tensor(-0.0632, grad_fn=<MeanBackward0>)\n","0.107\n","-0.09180898219347 + 0.0368216298520565 + 0.002040240913629532 = tensor(-0.0529, grad_fn=<MeanBackward0>)\n","0.092\n","-0.10721860080957413 + 0.036076102405786514 + 0.00204374804161489 = tensor(-0.0691, grad_fn=<MeanBackward0>)\n","0.112\n","-0.10212392359972 + 0.03714472055435181 + 0.0020382595248520374 = tensor(-0.0629, grad_fn=<MeanBackward0>)\n","0.116\n","-0.09566470235586166 + 0.040712397545576096 + 0.002043285872787237 = tensor(-0.0529, grad_fn=<MeanBackward0>)\n","0.08\n","Train Epoch: 6 [20000/60000 (33%)]\tLoss: -0.052909\n","-0.10097701847553253 + 0.03787386789917946 + 0.0020436428021639585 = tensor(-0.0611, grad_fn=<MeanBackward0>)\n","0.11\n","-0.10517483949661255 + 0.029607610777020454 + 0.0020429191645234823 = tensor(-0.0735, grad_fn=<MeanBackward0>)\n","0.099\n","-0.10283151268959045 + 0.05144444853067398 + 0.002043576445430517 = tensor(-0.0493, grad_fn=<MeanBackward0>)\n","0.096\n","-0.10581853985786438 + 0.035077277570962906 + 0.002040934283286333 = tensor(-0.0687, grad_fn=<MeanBackward0>)\n","0.106\n","-0.10081058740615845 + 0.03802605718374252 + 0.0020428893622010946 = tensor(-0.0607, grad_fn=<MeanBackward0>)\n","0.11\n","-0.09386559575796127 + 0.03748757764697075 + 0.0020468567963689566 = tensor(-0.0543, grad_fn=<MeanBackward0>)\n","0.083\n","-0.1028720960021019 + 0.05050540715456009 + 0.002044825814664364 = tensor(-0.0503, grad_fn=<MeanBackward0>)\n","0.105\n","-0.094323068857193 + 0.03338715806603432 + 0.0020450977608561516 = tensor(-0.0589, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10650384426116943 + 0.040352702140808105 + 0.002041494706645608 = tensor(-0.0641, grad_fn=<MeanBackward0>)\n","0.115\n","-0.0986679196357727 + 0.04106452316045761 + 0.002041163621470332 = tensor(-0.0556, grad_fn=<MeanBackward0>)\n","0.102\n","Train Epoch: 6 [30000/60000 (50%)]\tLoss: -0.055562\n","-0.0965275689959526 + 0.039287082850933075 + 0.002041252562776208 = tensor(-0.0552, grad_fn=<MeanBackward0>)\n","0.096\n","-0.09671579301357269 + 0.04183132201433182 + 0.0020455806516110897 = tensor(-0.0528, grad_fn=<MeanBackward0>)\n","0.09\n","-0.10167243331670761 + 0.03141513094305992 + 0.002039391780272126 = tensor(-0.0682, grad_fn=<MeanBackward0>)\n","0.11\n","-0.10270695388317108 + 0.043938808143138885 + 0.002045325469225645 = tensor(-0.0567, grad_fn=<MeanBackward0>)\n","0.102\n","-0.10079801827669144 + 0.04057719185948372 + 0.0020358378533273935 = tensor(-0.0582, grad_fn=<MeanBackward0>)\n","0.101\n","-0.0899372473359108 + 0.04157733917236328 + 0.0020412588492035866 = tensor(-0.0463, grad_fn=<MeanBackward0>)\n","0.096\n","-0.10071321576833725 + 0.03781803324818611 + 0.002044264692813158 = tensor(-0.0609, grad_fn=<MeanBackward0>)\n","0.089\n","-0.10129229724407196 + 0.033543601632118225 + 0.0020331901032477617 = tensor(-0.0657, grad_fn=<MeanBackward0>)\n","0.102\n","-0.09986104816198349 + 0.03626219555735588 + 0.0020391454454511404 = tensor(-0.0616, grad_fn=<MeanBackward0>)\n","0.092\n","-0.10441536456346512 + 0.03830277919769287 + 0.0020371433347463608 = tensor(-0.0641, grad_fn=<MeanBackward0>)\n","0.099\n","Train Epoch: 6 [40000/60000 (67%)]\tLoss: -0.064075\n","-0.09598693251609802 + 0.03175154700875282 + 0.002041216939687729 = tensor(-0.0622, grad_fn=<MeanBackward0>)\n","0.091\n","-0.09915097057819366 + 0.04221944138407707 + 0.0020452821627259254 = tensor(-0.0549, grad_fn=<MeanBackward0>)\n","0.099\n","-0.1042552962899208 + 0.04373537749052048 + 0.0020408036652952433 = tensor(-0.0585, grad_fn=<MeanBackward0>)\n","0.103\n","-0.10022995620965958 + 0.04039197787642479 + 0.002040350344032049 = tensor(-0.0578, grad_fn=<MeanBackward0>)\n","0.093\n","-0.10402406007051468 + 0.04613950476050377 + 0.0020392672158777714 = tensor(-0.0558, grad_fn=<MeanBackward0>)\n","0.103\n","-0.10261263698339462 + 0.03524000197649002 + 0.002045044908300042 = tensor(-0.0653, grad_fn=<MeanBackward0>)\n","0.116\n","-0.10052715986967087 + 0.030466370284557343 + 0.002044473309069872 = tensor(-0.0680, grad_fn=<MeanBackward0>)\n","0.109\n","-0.10801364481449127 + 0.03620311990380287 + 0.002047295216470957 = tensor(-0.0698, grad_fn=<MeanBackward0>)\n","0.109\n","-0.10121835023164749 + 0.041161470115184784 + 0.002044151071459055 = tensor(-0.0580, grad_fn=<MeanBackward0>)\n","0.099\n","-0.09827245771884918 + 0.03741423413157463 + 0.00204703351482749 = tensor(-0.0588, grad_fn=<MeanBackward0>)\n","0.083\n","Train Epoch: 6 [50000/60000 (83%)]\tLoss: -0.058811\n","-0.10071361809968948 + 0.04163050651550293 + 0.0020466072019189596 = tensor(-0.0570, grad_fn=<MeanBackward0>)\n","0.111\n","-0.10462315380573273 + 0.03649431839585304 + 0.002048956695944071 = tensor(-0.0661, grad_fn=<MeanBackward0>)\n","0.093\n","-0.0991346687078476 + 0.043381351977586746 + 0.002049128757789731 = tensor(-0.0537, grad_fn=<MeanBackward0>)\n","0.1\n","-0.09653334319591522 + 0.03694400191307068 + 0.002048421185463667 = tensor(-0.0575, grad_fn=<MeanBackward0>)\n","0.094\n","-0.09822981059551239 + 0.03854094073176384 + 0.002052963012829423 = tensor(-0.0576, grad_fn=<MeanBackward0>)\n","0.097\n","-0.09480071812868118 + 0.036430999636650085 + 0.0020486360881477594 = tensor(-0.0563, grad_fn=<MeanBackward0>)\n","0.094\n","-0.0953192487359047 + 0.04527224972844124 + 0.0020448060240596533 = tensor(-0.0480, grad_fn=<MeanBackward0>)\n","0.084\n","-0.09173957258462906 + 0.04126056656241417 + 0.0020466004498302937 = tensor(-0.0484, grad_fn=<MeanBackward0>)\n","0.089\n","-0.09177710860967636 + 0.04278365522623062 + 0.002045989967882633 = tensor(-0.0469, grad_fn=<MeanBackward0>)\n","0.085\n","-0.10435927659273148 + 0.03638775646686554 + 0.0020476800855249166 = tensor(-0.0659, grad_fn=<MeanBackward0>)\n","-0.10069207102060318 + 0.04021969810128212 + 0.002047950867563486 = tensor(-0.0584, grad_fn=<MeanBackward0>)\n","-0.1047079861164093 + 0.04138290882110596 + 0.0020461648236960173 = tensor(-0.0613, grad_fn=<MeanBackward0>)\n","-0.09357182681560516 + 0.0441211499273777 + 0.002047258894890547 = tensor(-0.0474, grad_fn=<MeanBackward0>)\n","-0.10414832085371017 + 0.0336163304746151 + 0.002045249566435814 = tensor(-0.0685, grad_fn=<MeanBackward0>)\n","-0.10269086062908173 + 0.031023217365145683 + 0.002047136193141341 = tensor(-0.0696, grad_fn=<MeanBackward0>)\n","-0.10953265428543091 + 0.033015549182891846 + 0.002047067042440176 = tensor(-0.0745, grad_fn=<MeanBackward0>)\n","-0.09961855411529541 + 0.03433872386813164 + 0.0020559264812618494 = tensor(-0.0632, grad_fn=<MeanBackward0>)\n","-0.10784035921096802 + 0.03404003009200096 + 0.00205013295635581 = tensor(-0.0718, grad_fn=<MeanBackward0>)\n","-0.09408987313508987 + 0.04169272258877754 + 0.00205048150382936 = tensor(-0.0503, grad_fn=<MeanBackward0>)\n","\n","Test set : Average loss : -0.0000, Accuracy: 1.031/10000 (0%)\n","\n","-0.10100718587636948 + 0.043189678341150284 + 0.0020427783019840717 = tensor(-0.0558, grad_fn=<MeanBackward0>)\n","0.098\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: -0.055775\n","-0.10242205858230591 + 0.034602344036102295 + 0.0020394332241266966 = tensor(-0.0658, grad_fn=<MeanBackward0>)\n","0.109\n","-0.10907156765460968 + 0.04119332134723663 + 0.0020443962421268225 = tensor(-0.0658, grad_fn=<MeanBackward0>)\n","0.1\n","-0.1075478047132492 + 0.03963083401322365 + 0.0020448192954063416 = tensor(-0.0659, grad_fn=<MeanBackward0>)\n","0.105\n","-0.09979609400033951 + 0.03686416149139404 + 0.0020368932746350765 = tensor(-0.0609, grad_fn=<MeanBackward0>)\n","0.108\n","-0.10111647099256516 + 0.03927144780755043 + 0.002042958978563547 = tensor(-0.0598, grad_fn=<MeanBackward0>)\n","0.099\n","-0.10520864278078079 + 0.03974222019314766 + 0.002044357592239976 = tensor(-0.0634, grad_fn=<MeanBackward0>)\n","0.095\n","-0.10265173017978668 + 0.03943084552884102 + 0.0020418954081833363 = tensor(-0.0612, grad_fn=<MeanBackward0>)\n","0.109\n","-0.09609263390302658 + 0.03725423663854599 + 0.0020424542017281055 = tensor(-0.0568, grad_fn=<MeanBackward0>)\n","0.082\n","-0.09949789196252823 + 0.04181603342294693 + 0.0020474442280828953 = tensor(-0.0556, grad_fn=<MeanBackward0>)\n","0.086\n","-0.10335098206996918 + 0.04182418808341026 + 0.0020415487233549356 = tensor(-0.0595, grad_fn=<MeanBackward0>)\n","0.114\n","Train Epoch: 7 [10000/60000 (17%)]\tLoss: -0.059485\n","-0.10104870051145554 + 0.04269901663064957 + 0.0020385142415761948 = tensor(-0.0563, grad_fn=<MeanBackward0>)\n","0.099\n","-0.10213603079319 + 0.04610864445567131 + 0.00204031472094357 = tensor(-0.0540, grad_fn=<MeanBackward0>)\n","0.108\n","-0.10391303896903992 + 0.0481826476752758 + 0.0020449019502848387 = tensor(-0.0537, grad_fn=<MeanBackward0>)\n","0.099\n","-0.10236513614654541 + 0.036807797849178314 + 0.0020388818811625242 = tensor(-0.0635, grad_fn=<MeanBackward0>)\n","0.091\n","-0.10532836616039276 + 0.04257553070783615 + 0.0020383785013109446 = tensor(-0.0607, grad_fn=<MeanBackward0>)\n","0.111\n","-0.10021776705980301 + 0.03464670479297638 + 0.0020429149735718966 = tensor(-0.0635, grad_fn=<MeanBackward0>)\n","0.102\n","-0.10559552907943726 + 0.03390244022011757 + 0.00204167771153152 = tensor(-0.0697, grad_fn=<MeanBackward0>)\n","0.123\n","-0.09931294620037079 + 0.03871995210647583 + 0.0020446034613996744 = tensor(-0.0585, grad_fn=<MeanBackward0>)\n","0.096\n","-0.09705576300621033 + 0.03972563147544861 + 0.002041466301307082 = tensor(-0.0553, grad_fn=<MeanBackward0>)\n","0.096\n","-0.10229335725307465 + 0.025796039029955864 + 0.00204841117374599 = tensor(-0.0744, grad_fn=<MeanBackward0>)\n","0.09\n","Train Epoch: 7 [20000/60000 (33%)]\tLoss: -0.074449\n","-0.10374154895544052 + 0.03302876278758049 + 0.0020435662008821964 = tensor(-0.0687, grad_fn=<MeanBackward0>)\n","0.097\n","-0.0980939269065857 + 0.0364200621843338 + 0.002039975253865123 = tensor(-0.0596, grad_fn=<MeanBackward0>)\n","0.099\n","-0.09910950809717178 + 0.03995079919695854 + 0.002042378531768918 = tensor(-0.0571, grad_fn=<MeanBackward0>)\n","0.092\n","-0.10085135698318481 + 0.03446631506085396 + 0.002043068641796708 = tensor(-0.0643, grad_fn=<MeanBackward0>)\n","0.101\n","-0.1052260771393776 + 0.0509134940803051 + 0.002050546230748296 = tensor(-0.0523, grad_fn=<MeanBackward0>)\n","0.115\n","-0.10090991109609604 + 0.038763515651226044 + 0.0020450311712920666 = tensor(-0.0601, grad_fn=<MeanBackward0>)\n","0.106\n","-0.09457936137914658 + 0.03648337349295616 + 0.0020430563017725945 = tensor(-0.0561, grad_fn=<MeanBackward0>)\n","0.09\n","-0.10247879475355148 + 0.03838855400681496 + 0.002046351321041584 = tensor(-0.0620, grad_fn=<MeanBackward0>)\n","0.111\n","-0.09704982489347458 + 0.03739975765347481 + 0.0020443371031433344 = tensor(-0.0576, grad_fn=<MeanBackward0>)\n","0.093\n","-0.10222259163856506 + 0.03258780390024185 + 0.0020465923007577658 = tensor(-0.0676, grad_fn=<MeanBackward0>)\n","0.116\n","Train Epoch: 7 [30000/60000 (50%)]\tLoss: -0.067588\n","-0.10742160677909851 + 0.03379817306995392 + 0.002048627706244588 = tensor(-0.0716, grad_fn=<MeanBackward0>)\n","0.107\n","-0.1043405756354332 + 0.03441762924194336 + 0.0020438076462596655 = tensor(-0.0679, grad_fn=<MeanBackward0>)\n","0.093\n","-0.09182673692703247 + 0.04638843610882759 + 0.0020476202480494976 = tensor(-0.0434, grad_fn=<MeanBackward0>)\n","0.091\n","-0.10378652811050415 + 0.02970038540661335 + 0.002047136891633272 = tensor(-0.0720, grad_fn=<MeanBackward0>)\n","0.108\n","-0.09489580243825912 + 0.02742108702659607 + 0.002050979295745492 = tensor(-0.0654, grad_fn=<MeanBackward0>)\n","0.095\n","-0.09417109936475754 + 0.04190735146403313 + 0.0020496107172220945 = tensor(-0.0502, grad_fn=<MeanBackward0>)\n","0.091\n","-0.09563096612691879 + 0.032599762082099915 + 0.0020486435387283564 = tensor(-0.0610, grad_fn=<MeanBackward0>)\n","0.088\n","-0.10188005119562149 + 0.03509215638041496 + 0.002049023052677512 = tensor(-0.0647, grad_fn=<MeanBackward0>)\n","0.094\n","-0.09111970663070679 + 0.03827318921685219 + 0.002045990666374564 = tensor(-0.0508, grad_fn=<MeanBackward0>)\n","0.083\n","-0.09996931254863739 + 0.04102200269699097 + 0.0020481536630541086 = tensor(-0.0569, grad_fn=<MeanBackward0>)\n","0.099\n","Train Epoch: 7 [40000/60000 (67%)]\tLoss: -0.056899\n","-0.11431851983070374 + 0.049500156193971634 + 0.0020448309369385242 = tensor(-0.0628, grad_fn=<MeanBackward0>)\n","0.103\n","-0.10649097710847855 + 0.03895590826869011 + 0.002046955516561866 = tensor(-0.0655, grad_fn=<MeanBackward0>)\n","0.106\n","-0.09966305643320084 + 0.0388951376080513 + 0.00203929515555501 = tensor(-0.0587, grad_fn=<MeanBackward0>)\n","0.09\n","-0.10484082996845245 + 0.04172905534505844 + 0.0020409158896654844 = tensor(-0.0611, grad_fn=<MeanBackward0>)\n","0.093\n","-0.10135923326015472 + 0.0357341542840004 + 0.002040787134319544 = tensor(-0.0636, grad_fn=<MeanBackward0>)\n","0.106\n","-0.10232560336589813 + 0.04708205908536911 + 0.002044445602223277 = tensor(-0.0532, grad_fn=<MeanBackward0>)\n","0.111\n","-0.10218815505504608 + 0.04495038092136383 + 0.002041434869170189 = tensor(-0.0552, grad_fn=<MeanBackward0>)\n","0.115\n","-0.10011029988527298 + 0.03125210851430893 + 0.0020413442980498075 = tensor(-0.0668, grad_fn=<MeanBackward0>)\n","0.099\n","-0.09488146752119064 + 0.03796030953526497 + 0.0020409079734236 = tensor(-0.0549, grad_fn=<MeanBackward0>)\n","0.08\n","-0.10377968847751617 + 0.032165031880140305 + 0.0020374928135424852 = tensor(-0.0696, grad_fn=<MeanBackward0>)\n","0.116\n","Train Epoch: 7 [50000/60000 (83%)]\tLoss: -0.069577\n","-0.10175664722919464 + 0.041809577494859695 + 0.002038479782640934 = tensor(-0.0579, grad_fn=<MeanBackward0>)\n","0.096\n","-0.09604968875646591 + 0.035944391041994095 + 0.0020412812009453773 = tensor(-0.0581, grad_fn=<MeanBackward0>)\n","0.094\n","-0.10544736683368683 + 0.04113820940256119 + 0.002037815051153302 = tensor(-0.0623, grad_fn=<MeanBackward0>)\n","0.101\n","-0.10263441503047943 + 0.04207294434309006 + 0.0020400804933160543 = tensor(-0.0585, grad_fn=<MeanBackward0>)\n","0.111\n","-0.09324134886264801 + 0.04047136381268501 + 0.002039397368207574 = tensor(-0.0507, grad_fn=<MeanBackward0>)\n","0.091\n","-0.10221842676401138 + 0.04381619766354561 + 0.002038809238001704 = tensor(-0.0564, grad_fn=<MeanBackward0>)\n","0.114\n","-0.10148420929908752 + 0.045154593884944916 + 0.0020370983984321356 = tensor(-0.0543, grad_fn=<MeanBackward0>)\n","0.103\n","-0.10532210022211075 + 0.04022366926074028 + 0.0020384842064231634 = tensor(-0.0631, grad_fn=<MeanBackward0>)\n","0.115\n","-0.10202223062515259 + 0.030832862481474876 + 0.0020324436482042074 = tensor(-0.0692, grad_fn=<MeanBackward0>)\n","0.112\n","-0.09758071601390839 + 0.030777854844927788 + 0.002034651581197977 = tensor(-0.0648, grad_fn=<MeanBackward0>)\n","-0.1043194979429245 + 0.044719815254211426 + 0.002035735175013542 = tensor(-0.0576, grad_fn=<MeanBackward0>)\n","-0.1040717288851738 + 0.048012875020504 + 0.002033664844930172 = tensor(-0.0540, grad_fn=<MeanBackward0>)\n","-0.1020020917057991 + 0.0374247170984745 + 0.002039523795247078 = tensor(-0.0625, grad_fn=<MeanBackward0>)\n","-0.10184404253959656 + 0.04368217661976814 + 0.0020405149552971125 = tensor(-0.0561, grad_fn=<MeanBackward0>)\n","-0.10240975767374039 + 0.03882865235209465 + 0.0020368604455143213 = tensor(-0.0615, grad_fn=<MeanBackward0>)\n","-0.09400776773691177 + 0.03750515729188919 + 0.0020379177294671535 = tensor(-0.0545, grad_fn=<MeanBackward0>)\n","-0.10089456290006638 + 0.043150294572114944 + 0.0020342981442809105 = tensor(-0.0557, grad_fn=<MeanBackward0>)\n","-0.09861750155687332 + 0.04518018662929535 + 0.0020448241848498583 = tensor(-0.0514, grad_fn=<MeanBackward0>)\n","-0.09988456964492798 + 0.03539222851395607 + 0.0020383542869240046 = tensor(-0.0625, grad_fn=<MeanBackward0>)\n","\n","Test set : Average loss : -0.0000, Accuracy: 1.003/10000 (0%)\n","\n","-0.10095147043466568 + 0.04051505774259567 + 0.0020345114171504974 = tensor(-0.0584, grad_fn=<MeanBackward0>)\n","0.107\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: -0.058402\n","-0.09720641374588013 + 0.034972913563251495 + 0.002036986406892538 = tensor(-0.0602, grad_fn=<MeanBackward0>)\n","0.092\n","-0.10182312875986099 + 0.034574855118989944 + 0.0020369021221995354 = tensor(-0.0652, grad_fn=<MeanBackward0>)\n","0.102\n","-0.10393854230642319 + 0.030247734859585762 + 0.0020361891947686672 = tensor(-0.0717, grad_fn=<MeanBackward0>)\n","0.122\n","-0.10136519372463226 + 0.03889116272330284 + 0.002039173152297735 = tensor(-0.0604, grad_fn=<MeanBackward0>)\n","0.104\n","-0.09551676362752914 + 0.03394326567649841 + 0.002035223413258791 = tensor(-0.0595, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10434567928314209 + 0.04684712737798691 + 0.0020367938559502363 = tensor(-0.0555, grad_fn=<MeanBackward0>)\n","0.119\n","-0.09832414984703064 + 0.040963638573884964 + 0.0020345421507954597 = tensor(-0.0553, grad_fn=<MeanBackward0>)\n","0.085\n","-0.09893900156021118 + 0.044265326112508774 + 0.002033660653978586 = tensor(-0.0526, grad_fn=<MeanBackward0>)\n","0.094\n","-0.10026505589485168 + 0.03294144943356514 + 0.0020342101342976093 = tensor(-0.0653, grad_fn=<MeanBackward0>)\n","0.078\n","-0.09493202716112137 + 0.03773504123091698 + 0.002036035293713212 = tensor(-0.0552, grad_fn=<MeanBackward0>)\n","0.093\n","Train Epoch: 8 [10000/60000 (17%)]\tLoss: -0.055161\n","-0.09492719918489456 + 0.03740474209189415 + 0.002036374295130372 = tensor(-0.0555, grad_fn=<MeanBackward0>)\n","0.089\n","-0.10168793052434921 + 0.03707081824541092 + 0.002038325648754835 = tensor(-0.0626, grad_fn=<MeanBackward0>)\n","0.106\n","-0.10088053345680237 + 0.03585509583353996 + 0.002034322824329138 = tensor(-0.0630, grad_fn=<MeanBackward0>)\n","0.099\n","-0.10068994760513306 + 0.03691805154085159 + 0.0020368676632642746 = tensor(-0.0617, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10377278923988342 + 0.03673800826072693 + 0.0020296452566981316 = tensor(-0.0650, grad_fn=<MeanBackward0>)\n","0.102\n","-0.10247589647769928 + 0.03480343148112297 + 0.002031427575275302 = tensor(-0.0656, grad_fn=<MeanBackward0>)\n","0.099\n","-0.10525499284267426 + 0.03973466157913208 + 0.0020341789349913597 = tensor(-0.0635, grad_fn=<MeanBackward0>)\n","0.114\n","-0.0966767966747284 + 0.0362103208899498 + 0.002035927027463913 = tensor(-0.0584, grad_fn=<MeanBackward0>)\n","0.099\n","-0.10046068578958511 + 0.03273484483361244 + 0.0020303691271692514 = tensor(-0.0657, grad_fn=<MeanBackward0>)\n","0.095\n","-0.10173086076974869 + 0.0373043492436409 + 0.0020363437943160534 = tensor(-0.0624, grad_fn=<MeanBackward0>)\n","0.102\n","Train Epoch: 8 [20000/60000 (33%)]\tLoss: -0.062390\n","-0.10418235510587692 + 0.038163185119628906 + 0.00203343341127038 = tensor(-0.0640, grad_fn=<MeanBackward0>)\n","0.109\n","-0.10505670309066772 + 0.034569572657346725 + 0.002036444377154112 = tensor(-0.0685, grad_fn=<MeanBackward0>)\n","0.117\n","-0.10283064842224121 + 0.035362325608730316 + 0.002039223676547408 = tensor(-0.0654, grad_fn=<MeanBackward0>)\n","0.095\n","-0.10452631115913391 + 0.04062262922525406 + 0.002041736152023077 = tensor(-0.0619, grad_fn=<MeanBackward0>)\n","0.11\n","-0.10107772052288055 + 0.029123809188604355 + 0.0020409701392054558 = tensor(-0.0699, grad_fn=<MeanBackward0>)\n","0.096\n","-0.0989961102604866 + 0.03292321786284447 + 0.0020416195038706064 = tensor(-0.0640, grad_fn=<MeanBackward0>)\n","0.113\n","-0.10102521628141403 + 0.036206260323524475 + 0.002041320549324155 = tensor(-0.0628, grad_fn=<MeanBackward0>)\n","0.108\n","-0.0986630991101265 + 0.03909885883331299 + 0.0020455580670386553 = tensor(-0.0575, grad_fn=<MeanBackward0>)\n","0.107\n","-0.09941898286342621 + 0.027413686737418175 + 0.0020471212919801474 = tensor(-0.0700, grad_fn=<MeanBackward0>)\n","0.093\n","-0.09290078282356262 + 0.045965131372213364 + 0.002050710842013359 = tensor(-0.0449, grad_fn=<MeanBackward0>)\n","0.089\n","Train Epoch: 8 [30000/60000 (50%)]\tLoss: -0.044885\n","-0.10055354237556458 + 0.03819639980792999 + 0.002049065660685301 = tensor(-0.0603, grad_fn=<MeanBackward0>)\n","0.106\n","-0.10453813523054123 + 0.03312871977686882 + 0.002049810951575637 = tensor(-0.0694, grad_fn=<MeanBackward0>)\n","0.115\n","-0.09377229958772659 + 0.03888793662190437 + 0.002052165800705552 = tensor(-0.0528, grad_fn=<MeanBackward0>)\n","0.092\n","-0.097720205783844 + 0.042183320969343185 + 0.0020494170021265745 = tensor(-0.0535, grad_fn=<MeanBackward0>)\n","0.107\n","-0.09919726848602295 + 0.030308106914162636 + 0.0020502193365246058 = tensor(-0.0668, grad_fn=<MeanBackward0>)\n","0.089\n","-0.10810045897960663 + 0.03611262887716293 + 0.0020508896559476852 = tensor(-0.0699, grad_fn=<MeanBackward0>)\n","0.109\n","-0.10042062401771545 + 0.03635589778423309 + 0.0020509061869233847 = tensor(-0.0620, grad_fn=<MeanBackward0>)\n","0.092\n","-0.10677816718816757 + 0.03714539110660553 + 0.0020542636048048735 = tensor(-0.0676, grad_fn=<MeanBackward0>)\n","0.104\n","-0.10261610150337219 + 0.034075405448675156 + 0.002053606789559126 = tensor(-0.0665, grad_fn=<MeanBackward0>)\n","0.106\n","-0.08898382633924484 + 0.03658305108547211 + 0.00205032411031425 = tensor(-0.0504, grad_fn=<MeanBackward0>)\n","0.087\n","Train Epoch: 8 [40000/60000 (67%)]\tLoss: -0.050350\n","-0.09197209030389786 + 0.030898699536919594 + 0.0020503830164670944 = tensor(-0.0590, grad_fn=<MeanBackward0>)\n","0.081\n","-0.1044502779841423 + 0.04537902772426605 + 0.0020553204230964184 = tensor(-0.0570, grad_fn=<MeanBackward0>)\n","0.107\n","-0.09770334511995316 + 0.044129498302936554 + 0.0020516482181847095 = tensor(-0.0515, grad_fn=<MeanBackward0>)\n","0.102\n","-0.10420840233564377 + 0.0399148091673851 + 0.0020559513941407204 = tensor(-0.0622, grad_fn=<MeanBackward0>)\n","0.102\n","-0.09932071715593338 + 0.04139716923236847 + 0.0020535101648420095 = tensor(-0.0559, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10610917955636978 + 0.04600261524319649 + 0.0020530575420707464 = tensor(-0.0581, grad_fn=<MeanBackward0>)\n","0.108\n","-0.10249961912631989 + 0.03548278659582138 + 0.0020502349361777306 = tensor(-0.0650, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10100435465574265 + 0.03483833372592926 + 0.002050760667771101 = tensor(-0.0641, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10682687908411026 + 0.03992241621017456 + 0.0020484691485762596 = tensor(-0.0649, grad_fn=<MeanBackward0>)\n","0.102\n","-0.09852192550897598 + 0.03539012372493744 + 0.002047056332230568 = tensor(-0.0611, grad_fn=<MeanBackward0>)\n","0.085\n","Train Epoch: 8 [50000/60000 (83%)]\tLoss: -0.061085\n","-0.09953144937753677 + 0.04055901989340782 + 0.00204831943847239 = tensor(-0.0569, grad_fn=<MeanBackward0>)\n","0.094\n","-0.09948466718196869 + 0.039135005325078964 + 0.002052686642855406 = tensor(-0.0583, grad_fn=<MeanBackward0>)\n","0.098\n","-0.1002369076013565 + 0.035785697400569916 + 0.0020479727536439896 = tensor(-0.0624, grad_fn=<MeanBackward0>)\n","0.1\n","-0.09926258772611618 + 0.029858622699975967 + 0.0020470928866416216 = tensor(-0.0674, grad_fn=<MeanBackward0>)\n","0.101\n","-0.09662521630525589 + 0.03185824677348137 + 0.0020420763175934553 = tensor(-0.0627, grad_fn=<MeanBackward0>)\n","0.095\n","-0.09823310375213623 + 0.04284900426864624 + 0.0020445159170776606 = tensor(-0.0533, grad_fn=<MeanBackward0>)\n","0.102\n","-0.1011529341340065 + 0.040452077984809875 + 0.002044007647782564 = tensor(-0.0587, grad_fn=<MeanBackward0>)\n","0.101\n","-0.1003599688410759 + 0.040044549852609634 + 0.0020417524501681328 = tensor(-0.0583, grad_fn=<MeanBackward0>)\n","0.103\n","-0.09742768108844757 + 0.041429657489061356 + 0.0020443275570869446 = tensor(-0.0540, grad_fn=<MeanBackward0>)\n","0.088\n","-0.09565702080726624 + 0.03198903426527977 + 0.0020414807368069887 = tensor(-0.0616, grad_fn=<MeanBackward0>)\n","-0.09926488995552063 + 0.04230230674147606 + 0.002045967383310199 = tensor(-0.0549, grad_fn=<MeanBackward0>)\n","-0.0945483073592186 + 0.03345043212175369 + 0.0020504388958215714 = tensor(-0.0590, grad_fn=<MeanBackward0>)\n","-0.09205599874258041 + 0.03726784884929657 + 0.002049218863248825 = tensor(-0.0527, grad_fn=<MeanBackward0>)\n","-0.10164870321750641 + 0.030193021520972252 + 0.002050211885944009 = tensor(-0.0694, grad_fn=<MeanBackward0>)\n","-0.0963088870048523 + 0.045239854604005814 + 0.0020474593620747328 = tensor(-0.0490, grad_fn=<MeanBackward0>)\n","-0.09997101873159409 + 0.031165003776550293 + 0.0020450185984373093 = tensor(-0.0668, grad_fn=<MeanBackward0>)\n","-0.10654747486114502 + 0.04286402463912964 + 0.002043996937572956 = tensor(-0.0616, grad_fn=<MeanBackward0>)\n","-0.09818702936172485 + 0.041131820529699326 + 0.0020479473751038313 = tensor(-0.0550, grad_fn=<MeanBackward0>)\n","-0.1064777672290802 + 0.04249629005789757 + 0.0020430407021194696 = tensor(-0.0619, grad_fn=<MeanBackward0>)\n","\n","Test set : Average loss : -0.0000, Accuracy: 0.9679999999999999/10000 (0%)\n","\n","-0.10671065747737885 + 0.034114111214876175 + 0.0020439513027668 = tensor(-0.0706, grad_fn=<MeanBackward0>)\n","0.112\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: -0.070553\n","-0.09930635243654251 + 0.04234343022108078 + 0.002043853746727109 = tensor(-0.0549, grad_fn=<MeanBackward0>)\n","0.105\n","-0.10955198109149933 + 0.020365847274661064 + 0.002042846754193306 = tensor(-0.0871, grad_fn=<MeanBackward0>)\n","0.123\n","-0.09341980516910553 + 0.03531242534518242 + 0.002041863277554512 = tensor(-0.0561, grad_fn=<MeanBackward0>)\n","0.097\n","-0.10036017000675201 + 0.03066105581820011 + 0.0020455648191273212 = tensor(-0.0677, grad_fn=<MeanBackward0>)\n","0.095\n","-0.09675059467554092 + 0.03982461616396904 + 0.002045256085693836 = tensor(-0.0549, grad_fn=<MeanBackward0>)\n","0.086\n","-0.09698107838630676 + 0.0404876172542572 + 0.002044627210125327 = tensor(-0.0544, grad_fn=<MeanBackward0>)\n","0.092\n","-0.10032853484153748 + 0.037213750183582306 + 0.002045862842351198 = tensor(-0.0611, grad_fn=<MeanBackward0>)\n","0.096\n","-0.10141371190547943 + 0.03966156020760536 + 0.0020468172151595354 = tensor(-0.0597, grad_fn=<MeanBackward0>)\n","0.097\n","-0.10610473155975342 + 0.03536786884069443 + 0.0020441829692572355 = tensor(-0.0687, grad_fn=<MeanBackward0>)\n","0.105\n","-0.09839249402284622 + 0.03237350657582283 + 0.002041632542386651 = tensor(-0.0640, grad_fn=<MeanBackward0>)\n","0.108\n","Train Epoch: 9 [10000/60000 (17%)]\tLoss: -0.063977\n","-0.09906329959630966 + 0.034581150859594345 + 0.0020434244070202112 = tensor(-0.0624, grad_fn=<MeanBackward0>)\n","0.082\n","-0.10305526852607727 + 0.037090104073286057 + 0.002042587613686919 = tensor(-0.0639, grad_fn=<MeanBackward0>)\n","0.098\n","-0.09567219018936157 + 0.04345189407467842 + 0.0020442879758775234 = tensor(-0.0502, grad_fn=<MeanBackward0>)\n","0.105\n","-0.10032735019922256 + 0.03423074260354042 + 0.002039847429841757 = tensor(-0.0641, grad_fn=<MeanBackward0>)\n","0.103\n","-0.09881247580051422 + 0.036984700709581375 + 0.002043116372078657 = tensor(-0.0598, grad_fn=<MeanBackward0>)\n","0.092\n","-0.09276217222213745 + 0.038179874420166016 + 0.0020489965099841356 = tensor(-0.0525, grad_fn=<MeanBackward0>)\n","0.086\n","-0.09752705693244934 + 0.03975562006235123 + 0.002045046305283904 = tensor(-0.0557, grad_fn=<MeanBackward0>)\n","0.104\n","-0.0944225937128067 + 0.03139280155301094 + 0.002040016697719693 = tensor(-0.0610, grad_fn=<MeanBackward0>)\n","0.091\n","-0.09049370884895325 + 0.04200416058301926 + 0.0020431941375136375 = tensor(-0.0464, grad_fn=<MeanBackward0>)\n","0.083\n","-0.10422136634588242 + 0.040689706802368164 + 0.002041816245764494 = tensor(-0.0615, grad_fn=<MeanBackward0>)\n","0.109\n","Train Epoch: 9 [20000/60000 (33%)]\tLoss: -0.061490\n","-0.10481595247983932 + 0.03992968797683716 + 0.002040862338617444 = tensor(-0.0628, grad_fn=<MeanBackward0>)\n","0.112\n","-0.10141284018754959 + 0.03639277443289757 + 0.0020399598870426416 = tensor(-0.0630, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10414417833089828 + 0.04033849760890007 + 0.002047900343313813 = tensor(-0.0618, grad_fn=<MeanBackward0>)\n","0.097\n","-0.09864319115877151 + 0.03746979683637619 + 0.0020426353439688683 = tensor(-0.0591, grad_fn=<MeanBackward0>)\n","0.095\n","-0.09780705720186234 + 0.03675069659948349 + 0.002041175030171871 = tensor(-0.0590, grad_fn=<MeanBackward0>)\n","0.097\n","-0.10611564666032791 + 0.03472764790058136 + 0.002046118723228574 = tensor(-0.0693, grad_fn=<MeanBackward0>)\n","0.112\n","-0.09595206379890442 + 0.03101305663585663 + 0.002043950604274869 = tensor(-0.0629, grad_fn=<MeanBackward0>)\n","0.091\n","-0.10085924714803696 + 0.02992347627878189 + 0.0020440176595002413 = tensor(-0.0689, grad_fn=<MeanBackward0>)\n","0.106\n","-0.09588797390460968 + 0.036177732050418854 + 0.0020472968462854624 = tensor(-0.0577, grad_fn=<MeanBackward0>)\n","0.108\n","-0.10463251173496246 + 0.03662816807627678 + 0.0020477930083870888 = tensor(-0.0660, grad_fn=<MeanBackward0>)\n","0.105\n","Train Epoch: 9 [30000/60000 (50%)]\tLoss: -0.065957\n","-0.09008361399173737 + 0.03380077704787254 + 0.002046936424449086 = tensor(-0.0542, grad_fn=<MeanBackward0>)\n","0.085\n","-0.0987572968006134 + 0.03377305343747139 + 0.0020497646182775497 = tensor(-0.0629, grad_fn=<MeanBackward0>)\n","0.099\n","-0.10056677460670471 + 0.035941142588853836 + 0.002048484282568097 = tensor(-0.0626, grad_fn=<MeanBackward0>)\n","0.107\n","-0.09623254090547562 + 0.03312864899635315 + 0.0020480479579418898 = tensor(-0.0611, grad_fn=<MeanBackward0>)\n","0.09\n","-0.10388054698705673 + 0.034409888088703156 + 0.002048831433057785 = tensor(-0.0674, grad_fn=<MeanBackward0>)\n","0.109\n","-0.10123682767152786 + 0.0396171472966671 + 0.002048124559223652 = tensor(-0.0596, grad_fn=<MeanBackward0>)\n","0.099\n","-0.09614565968513489 + 0.03585733473300934 + 0.002048398833721876 = tensor(-0.0582, grad_fn=<MeanBackward0>)\n","0.085\n","-0.10050737112760544 + 0.03825293853878975 + 0.0020521802362054586 = tensor(-0.0602, grad_fn=<MeanBackward0>)\n","0.098\n","-0.10107129067182541 + 0.03306899592280388 + 0.0020479755476117134 = tensor(-0.0660, grad_fn=<MeanBackward0>)\n","0.102\n","-0.09951294213533401 + 0.029562287032604218 + 0.002050549490377307 = tensor(-0.0679, grad_fn=<MeanBackward0>)\n","0.111\n","Train Epoch: 9 [40000/60000 (67%)]\tLoss: -0.067900\n","-0.09880457073450089 + 0.03966517001390457 + 0.002052102703601122 = tensor(-0.0571, grad_fn=<MeanBackward0>)\n","0.098\n","-0.10345930606126785 + 0.03856424614787102 + 0.002056497847661376 = tensor(-0.0628, grad_fn=<MeanBackward0>)\n","0.106\n","-0.10176275670528412 + 0.046362970024347305 + 0.002052389085292816 = tensor(-0.0533, grad_fn=<MeanBackward0>)\n","0.109\n","-0.09723939746618271 + 0.025263361632823944 + 0.002053102944046259 = tensor(-0.0699, grad_fn=<MeanBackward0>)\n","0.101\n","-0.09843470901250839 + 0.036465588957071304 + 0.0020513636991381645 = tensor(-0.0599, grad_fn=<MeanBackward0>)\n","0.099\n","-0.09663473814725876 + 0.038850024342536926 + 0.002053071977570653 = tensor(-0.0557, grad_fn=<MeanBackward0>)\n","0.102\n","-0.1009100079536438 + 0.03946325555443764 + 0.0020584820304065943 = tensor(-0.0594, grad_fn=<MeanBackward0>)\n","0.1\n","-0.09674208611249924 + 0.03079746849834919 + 0.002058057812973857 = tensor(-0.0639, grad_fn=<MeanBackward0>)\n","0.094\n","-0.11294665932655334 + 0.04056688770651817 + 0.002050615381449461 = tensor(-0.0703, grad_fn=<MeanBackward0>)\n","0.116\n","-0.10379785299301147 + 0.03386355936527252 + 0.002053419826552272 = tensor(-0.0679, grad_fn=<MeanBackward0>)\n","0.099\n","Train Epoch: 9 [50000/60000 (83%)]\tLoss: -0.067881\n","-0.10018914937973022 + 0.03606759384274483 + 0.0020579241681843996 = tensor(-0.0621, grad_fn=<MeanBackward0>)\n","0.092\n","-0.10190411657094955 + 0.041366830468177795 + 0.002055325312539935 = tensor(-0.0585, grad_fn=<MeanBackward0>)\n","0.101\n","-0.10535167157649994 + 0.0347122959792614 + 0.002057785401120782 = tensor(-0.0686, grad_fn=<MeanBackward0>)\n","0.113\n","-0.09761705249547958 + 0.031809695065021515 + 0.002054737415164709 = tensor(-0.0638, grad_fn=<MeanBackward0>)\n","0.101\n","-0.10168169438838959 + 0.03445598855614662 + 0.0020559271797537804 = tensor(-0.0652, grad_fn=<MeanBackward0>)\n","0.102\n","-0.09962554275989532 + 0.036348532885313034 + 0.002059070859104395 = tensor(-0.0612, grad_fn=<MeanBackward0>)\n","0.108\n","-0.10344995558261871 + 0.032699648290872574 + 0.0020568796899169683 = tensor(-0.0687, grad_fn=<MeanBackward0>)\n","0.097\n","-0.09616152197122574 + 0.03898545727133751 + 0.002055289689451456 = tensor(-0.0551, grad_fn=<MeanBackward0>)\n","0.092\n","-0.10000202059745789 + 0.032906681299209595 + 0.002053674776107073 = tensor(-0.0650, grad_fn=<MeanBackward0>)\n","0.095\n","-0.09762158244848251 + 0.03965853527188301 + 0.0020558142568916082 = tensor(-0.0559, grad_fn=<MeanBackward0>)\n","-0.10544359683990479 + 0.03707839176058769 + 0.0020568561740219593 = tensor(-0.0663, grad_fn=<MeanBackward0>)\n","-0.10079704225063324 + 0.03720458969473839 + 0.002052662428468466 = tensor(-0.0615, grad_fn=<MeanBackward0>)\n","-0.11663798242807388 + 0.03343381732702255 + 0.002055382588878274 = tensor(-0.0811, grad_fn=<MeanBackward0>)\n","-0.10401198267936707 + 0.03339061886072159 + 0.0020582731813192368 = tensor(-0.0686, grad_fn=<MeanBackward0>)\n","-0.0998903140425682 + 0.03410499170422554 + 0.0020540610421448946 = tensor(-0.0637, grad_fn=<MeanBackward0>)\n","-0.09703598916530609 + 0.03052501194179058 + 0.0020601563155651093 = tensor(-0.0645, grad_fn=<MeanBackward0>)\n","-0.0968257412314415 + 0.035493556410074234 + 0.0020569083280861378 = tensor(-0.0593, grad_fn=<MeanBackward0>)\n","-0.10050893574953079 + 0.03546823933720589 + 0.002055648947134614 = tensor(-0.0630, grad_fn=<MeanBackward0>)\n","-0.09820348024368286 + 0.04006766155362129 + 0.0020588927436619997 = tensor(-0.0561, grad_fn=<MeanBackward0>)\n","\n","Test set : Average loss : -0.0000, Accuracy: 0.9889999999999999/10000 (0%)\n","\n","-0.10020294785499573 + 0.049332395195961 + 0.002058182144537568 = tensor(-0.0488, grad_fn=<MeanBackward0>)\n","0.092\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: -0.048812\n","-0.10157724469900131 + 0.039359480142593384 + 0.0020546745508909225 = tensor(-0.0602, grad_fn=<MeanBackward0>)\n","0.1\n","-0.10065232962369919 + 0.03859703987836838 + 0.002053080825135112 = tensor(-0.0600, grad_fn=<MeanBackward0>)\n","0.102\n","-0.09941662847995758 + 0.0322292186319828 + 0.0020518305245786905 = tensor(-0.0651, grad_fn=<MeanBackward0>)\n","0.099\n","-0.0967465415596962 + 0.029046988114714622 + 0.002051885472610593 = tensor(-0.0656, grad_fn=<MeanBackward0>)\n","0.099\n","-0.09764794260263443 + 0.03592394292354584 + 0.0020496887154877186 = tensor(-0.0597, grad_fn=<MeanBackward0>)\n","0.093\n","-0.10119722783565521 + 0.04312831163406372 + 0.002048867754638195 = tensor(-0.0560, grad_fn=<MeanBackward0>)\n","0.095\n","-0.10892877727746964 + 0.03962668031454086 + 0.0020471534226089716 = tensor(-0.0673, grad_fn=<MeanBackward0>)\n","0.113\n","-0.10036216676235199 + 0.02737273834645748 + 0.002043004846200347 = tensor(-0.0709, grad_fn=<MeanBackward0>)\n","0.101\n","-0.09677955508232117 + 0.029093792662024498 + 0.002043867949396372 = tensor(-0.0656, grad_fn=<MeanBackward0>)\n","0.092\n","-0.10170458257198334 + 0.040103424340486526 + 0.0020406837575137615 = tensor(-0.0596, grad_fn=<MeanBackward0>)\n","0.098\n","Train Epoch: 10 [10000/60000 (17%)]\tLoss: -0.059560\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-57598d8d62dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-a74500d9d454>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epoch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"zkXByH5vhw9n","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}