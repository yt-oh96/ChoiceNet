{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ChoiceNet.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOEyKdXg6TuJV4GjI7f5QLD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-rEvDt64VRWt","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"oMp2H62SpnW7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TnsBAqeMPtPg","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import math\n","import torch.nn.functional as F\n","import argparse\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","import torch.optim as optim\n","\n","\n","class ChoiceNet(nn.Module):\n","  def __init__(self, backbone, y_dim, num_mixture, feature_dim, logSigmaZval, tau_inv, pi1_bias):\n","    super(ChoiceNet,self).__init__()\n","    self.backbone = backbone\n","    self.y_dim = y_dim\n","    self.num_mixture = num_mixture\n","    self.feature_dim = feature_dim\n","    self.logSigmaZval = logSigmaZval\n","    self.tau_inv = tau_inv\n","    self.pi1_bias = pi1_bias\n","    self.rho_ref = 1\n","    #self.USE_GAT = USE_GAT\n","    \n","    \n","    self.fc_feature_dim = nn.Linear(7*7*64, self.feature_dim)\n","    self.fc_num_mixture = nn.Linear(self.feature_dim, num_mixture)\n","\n","    self.fc_var_raw = nn.Linear(self.feature_dim, self.y_dim)\n","\n","    self.fc_pi_logits = nn.Linear(self.feature_dim, self.num_mixture)\n","  def forward(self, x):\n","    x = self.backbone(x)\n","    x = x.view(x.size(0),-1) # flatten\n","    self.feature = self.fc_feature_dim(x) # feature, h\n","    rho_raw = self.fc_num_mixture(self.feature)\n","    rho_temp = F.sigmoid(rho_raw)\n","    \n","    rho = torch.cat([rho_temp[:, 0:1]*0.0 + self.rho_ref, rho_temp[:, 1:]], axis=1) # rho(h)=rho1~rhoK, rho_ref=1\n","\n","    Q = self.feature_dim\n","    num_data = x.size()[0]\n","\n","    #make_sample\n","    muW_tile, muZ_tile, sigmaW_tile, sigmaZ_tile = self.make_sample(Q, num_data) \n","    \n","    # cholesky #[K*N*Q*D]\n","    samplerList = self.cholesky(self.num_mixture, Q, rho, num_data, muW_tile, sigmaW_tile, muZ_tile, sigmaZ_tile)\n","\n","    wSample = samplerList.permute(1,3,0,2) #[N*D*K*Q]\n","\n","    #K mean mixtures\n","    wTemp = wSample.contiguous().view(num_data, self.num_mixture*self.y_dim, Q)\n","    #wTemp = torch.reshape(wSample,(wSample.shape[0],wSample.shape[1]*wSample.shape[2],wSample.shape[3])))\n","    #wTemp = wSample.view(num_data, self.num_mixture*self.y_dim, Q) #[N*DK*Q]\n","    featRsh = self.feature.view(num_data, Q, 1)\n","     \n","    _mu = torch.matmul(wTemp, featRsh) #[N*DK*1]\n","    mu = _mu.view(num_data, self.y_dim, self.num_mixture)\n","\n","    ### Add bias to mu (after)\n","\n","    #K var mixtures\n","    logvar_raw = self.fc_var_raw(self.feature) #[N*D]\n","    var_raw = torch.exp(logvar_raw)\n","    var_tile = var_raw.unsqueeze(2).repeat(1, 1, self.num_mixture)\n","    rho_tile = rho.unsqueeze(1).repeat(1, self.y_dim, 1)\n","    tau_inv = self.tau_inv\n","    var = (1.0 - torch.pow(rho_tile,2))*var_tile + tau_inv\n","\n","    # Weight allocation probability pi [N*K]\n","    pi_logits = self.fc_pi_logits(self.feature) #[N*K]\n","    pi_temp = F.softmax(pi_logits, dim=1)\n","\n","    pi_temp = torch.cat((pi_temp[:, 0:1] + self.pi1_bias, pi_temp[:, 1:]), axis=1)\n","\n","    pi = F.softmax(pi_temp, dim=1)\n","\n","    return rho, mu, var, pi\n","    \n","  def make_sample(self, Q, num_data):\n","    N = num_data\n","\n","    #muW = torch.nn.init.normal_(torch.empty(Q, self.y_dim), mean = 0.0, std = 0.1)\n","    muW = torch.randn(Q, self.y_dim, dtype=torch.float)\n","    logSigmaW = torch.nn.init.constant_(torch.empty(Q, self.y_dim), -3.0)\n","    \n","    muZ = torch.zeros(Q, self.y_dim)\n","    logSigmaZ = torch.nn.init.constant_(torch.empty(Q, self.y_dim), self.logSigmaZval)\n","    \n","    muW_tile = muW.unsqueeze(0).repeat(N,1,1)\n","    sigmaW_tile = torch.exp(logSigmaW.unsqueeze(0).repeat(N,1,1))\n","    \n","    muZ_tile = muZ.unsqueeze(0).repeat(N,1,1)\n","    sigmaZ_tile = torch.exp(logSigmaZ.unsqueeze(0).repeat(N,1,1))\n","\n","    return muW_tile, muZ_tile, sigmaW_tile, sigmaZ_tile\n","\n","  def cholesky(self, num_mixture, Q, rho, num_data, muW_tile, sigmaW_tile, muZ_tile, sigmaZ_tile):\n","    samplerList = []\n","    for mix_idx in range(self.num_mixture):\n","      rho_j = rho[:, mix_idx : mix_idx+1]\n","      rho_tile = rho_j.unsqueeze(2).repeat(1, Q, self.y_dim)\n","      \n","      #epsW = torch.nn.init.normal_(torch.empty(num_data, Q, self.y_dim), mean = 0, std = 1)  #[N,Q,D]\n","      epsW = torch.randn(num_data, Q, self.y_dim, dtype=torch.float)\n","      W = muW_tile + torch.sqrt(sigmaW_tile)*epsW\n","      \n","      #epsZ = torch.nn.init.normal_(torch.empty(num_data, Q, self.y_dim), mean = 0, std = 1)\n","      epsZ = torch.randn(num_data, Q, self.y_dim, dtype=torch.float)\n","      Z = muZ_tile + torch.sqrt(sigmaZ_tile)*epsZ\n","\n","      #Cholesky\n","      Y = rho_tile*muW_tile + (1.0 - torch.pow(rho_tile,2))\\\n","                               *(rho_tile*torch.sqrt(sigmaZ_tile)/torch.sqrt(sigmaW_tile)\\\n","                                 *(W - muW_tile) + Z*torch.sqrt(1 - torch.pow(rho_tile,2)))\n","      \n","      samplerList.append(Y)\n","    return torch.stack(samplerList)\n","\n","\n","    #muW_tile = \n","def make_layers(in_channels, h_dim, filter_size, max_pools, activation, batch_norm =False):\n","    layers = []\n","\n","    #mnist\n","    for h_idx in range(len(h_dim)):\n","      fs = filter_size[h_idx]\n","      hidden = h_dim[h_idx]\n","      conv2d = nn.Conv2d(in_channels, hidden, kernel_size = fs, padding = 1)\n","      if batch_norm:\n","        layers += [conv2d, nn.BatchNorm2d(hidden), activation]\n","      else:\n","        layers += [conv2d, activation]\n","      in_channels = hidden\n","     \n","      max_pool = max_pools[h_idx]\n","      if max_pool > 1:\n","        layers += [nn.MaxPool2d(max_pool)]\n","    return nn.Sequential(*layers)\n","\n","\n","def ChoiceNet_Mnist():\n","  return ChoiceNet(make_layers(in_channels, h_dim, filter_size, max_pools, activation, batch_norm ),\n","                   y_dim, num_mixture, feature_dim, logSigmaZval, tau_inv, pi1_bias)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0ChmT6jPyXN","colab_type":"code","colab":{}},"source":["\n","\n","\n","args = {}\n","kwargs = {}\n","args['batch_size'] = 1000\n","args['test_batch_size'] = 1000\n","args['epochs'] = 10\n","args['lr'] = 0.01\n","args['momentum'] = 0.5\n","\n","args['seed'] = 1\n","args['log_interval'] = 10\n","args['cuda'] = False\n","\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=True, download=True,\n","                   transform=transforms.Compose([\n","                                                 transforms.ToTensor(),\n","                                                 transforms.Normalize((0.1307,), (0.3081,))\n","                   ])),\n","                   batch_size = args['batch_size'], shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=False, download=True,\n","                   transform=transforms.Compose([\n","                                                 transforms.ToTensor(),\n","                                                 transforms.Normalize((0.1307,), (0.3081,))\n","                   ])),\n","                   batch_size = args['test_batch_size'], shuffle=True, **kwargs)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OQJTMfZ3cBiC","colab_type":"code","colab":{}},"source":["\n","\n","\n","def train(epoch):\n","  model.train()\n","  for batch_idx, (data,target) in enumerate(train_loader):\n","    if args['cuda']:\n","      data, target = data.cuda(), target.cuda()\n","    \n","    temp = torch.LongTensor(1000,1).random_() % 10\n","    y_onehot = torch.FloatTensor(1000,10)\n","\n","    y_onehot.zero_()\n","    y_onehot.scatter_(1, temp, 1)\n","    \n","    data, target = Variable(data), Variable(y_onehot)\n","    \n","    #GRAD_CLIP = True, USE_SGD = False\n","\n","    optimizer.zero_grad()\n","\n","    rho, mu, var, pi = model(data)\n","\n","    loss = MDNloss(len(data), rho, mu, var, pi, target, y_dim=10, num_mixture=10, logsumexp_coef= 1e-2, kl_reg_coef= 1e-4).forward()\n","    loss.requires_grad=True\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","    if batch_idx%args['log_interval'] == 0:\n","      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","          epoch, batch_idx * len(data), len(train_loader.dataset),\n","          100. *batch_idx/len(train_loader), loss.data\n","      ))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iDNEZ8qpeuhI","colab_type":"code","colab":{}},"source":["def MDNloss(num_data, rho, mu, var, pi, target, y_dim, num_mixture, logsumexp_coef, kl_reg_coef):\n","  yhat = mu + torch.sqrt(var)*torch.nn.init.normal_(torch.empty(num_data, y_dim, num_mixture))\n","  target_tile = target.unsqueeze(-1).repeat(1, 1, num_mixture)\n","  #target_tile = torch.unsqueeze(target,2)\n","  pi_tile = pi.unsqueeze(1).repeat(1, y_dim, 1)\n","\n","  yhat_normalized = F.softmax(yhat, dim=1)\n","  _loss_fit = torch.sum(pi_tile*yhat_normalized*target_tile, axis=[1,2])\n","  loss_fit = torch.mean(_loss_fit)\n","\n","  _loss_reg = pi*torch.logsumexp(yhat,axis=[1])\n","  __loss_reg = torch.sum(_loss_reg,axis=[1])\n","  loss_reg = logsumexp_coef*torch.mean(__loss_reg)\n","\n","  _eps = 1e-8\n","  _kl_reg = kl_reg_coef*torch.sum(rho*(torch.log(pi+_eps) - torch.log(rho+_eps)), axis=1)\n","  kl_reg = torch.mean(_kl_reg)\n","\n","  return torch.mean(loss_fit + loss_reg)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jZCQU8K4nsCp","colab_type":"code","colab":{}},"source":["import math\n","class MDNloss(nn.Module):\n","  def __init__(self, num_data, rho, mu, var, pi, target, y_dim, num_mixture, logsumexp_coef, kl_reg_coef):\n","    super(MDNloss, self).__init__()\n","    self.num_data = num_data\n","    self.rho = Variable(rho)\n","    self.mu = Variable(mu)\n","    self.var = Variable(var)\n","    self.pi = Variable(pi)\n","    self.target = Variable(target)\n","    self.y_dim = y_dim\n","    self.num_mixture = num_mixture\n","    self.logsumexp_coef = logsumexp_coef\n","    self.kl_reg_coef = kl_reg_coef\n","    self.yhat = self.mu + torch.sqrt(self.var)*torch.randn(self.num_data, self.y_dim, self.num_mixture)\n","\n","  def forward(self):\n","    target_tile = self.target.unsqueeze(-1).repeat(1, 1, self.num_mixture)\n","    #target_tile = torch.unsqueeze(target,2)\n","    pi_tile = self.pi.unsqueeze(1).repeat(1, self.y_dim, 1)\n","\n","    yhat_normalized = F.softmax(self.yhat, dim=1)\n","    _loss_fit = torch.sum(pi_tile*yhat_normalized*target_tile, axis=[1,2])\n","    loss_fit = torch.mean(_loss_fit)\n","\n","    _loss_reg = self.pi*torch.logsumexp(self.yhat,axis=[1])\n","    __loss_reg = torch.sum(_loss_reg,axis=[1])\n","    loss_reg = self.logsumexp_coef*torch.mean(__loss_reg)\n","\n","    _eps = 1e-8\n","    _kl_reg = self.kl_reg_coef*torch.sum(self.rho*(torch.log(self.pi+_eps) - torch.log(self.rho+_eps)), axis=1)\n","    kl_reg = torch.mean(_kl_reg)\n","    # prob = self.pi*self.g_p(self.var,self.mu,self.target)\n","    # nll = -torch.log(torch.sum(prob,dim=1))\n","\n","    return torch.mean(loss_fit + loss_reg)\n","\n","  def g_p(self, sigma, mu, target):\n","    ONEOVERSQRT2PI = 1.0/math.sqrt(2*math.pi)\n","    target = target.unsqueeze(1).expand_as(sigma)\n","    ret = ONEOVERSQRT2PI * torch.exp(-0.5*((target-mu)/sigma)**2)/sigma\n","    return torch.prod(ret,2)\n","\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WgsBQTzolR0y","colab_type":"code","outputId":"86d14d09-c6de-44b7-afbc-4d0aff1894eb","executionInfo":{"status":"ok","timestamp":1585834533069,"user_tz":-540,"elapsed":645434,"user":{"displayName":"‍오영택[ 대학원석·박사통합과정휴학 / 인공지능학과 ]","photoUrl":"","userId":"14804494411714270071"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = ChoiceNet(make_layers(in_channels=1, h_dim=[64,64], filter_size=[3,3], max_pools=[2,2], activation=torch.nn.ReLU(), batch_norm=True ),y_dim=10, num_mixture=10, feature_dim=128, logSigmaZval=-2, tau_inv=1e-4, pi1_bias=0.0)\n","\n","optimizer = optim.Adam(model.parameters(), lr=args['lr'])\n","\n","for epoch in range(1, args['epochs']+1):\n","  train(epoch)"],"execution_count":66,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.167755\n","Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.160112\n","Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.171089\n","Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.157319\n","Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.171341\n","Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.163990\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.169000\n","Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.179868\n","Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.165136\n","Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.155806\n","Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.186804\n","Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.177507\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.182849\n","Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.174369\n","Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.174780\n","Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.173424\n","Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.162308\n","Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.165289\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.159732\n","Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.172748\n","Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.166667\n","Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.188001\n","Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.172634\n","Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.177443\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.166565\n","Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.158804\n","Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.184844\n","Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.144699\n","Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.169314\n","Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.155615\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.161750\n","Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.167131\n","Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.190741\n","Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.186389\n","Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.173404\n","Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.166392\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.169382\n","Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.147584\n","Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.172278\n","Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.174027\n","Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.173953\n","Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.159819\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.162297\n","Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.165709\n","Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.174916\n","Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.141323\n","Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.186561\n","Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.183212\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.153436\n","Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.155905\n","Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.188343\n","Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.154359\n","Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.160887\n","Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.196705\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.190567\n","Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.148043\n","Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.167803\n","Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.169926\n","Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.183132\n","Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.158987\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"om-5ajgB8pHY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}